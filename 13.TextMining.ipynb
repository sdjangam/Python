{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "string = 'At Waterloo, where there were many trains, \\\n",
    "we were fortunate in catching a train for Leatherhead.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', ',', 'where', 'there', 'were', 'many', 'trains', ',', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'Leatherhead', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(string)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'where', 'there', 'were', 'many', 'trains', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'Leatherhead']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import RegexpTokenizer as regextoken\n",
    "tokenizer = regextoken(r'\\w+')\n",
    "tokens = tokenizer.tokenize(string)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'waterloo', 'where', 'there', 'were', 'many', 'trains', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'leatherhead'] "
     ]
    }
   ],
   "source": [
    "tokens = [token.lower() for token in tokens]\n",
    "print(tokens,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'many', 'trains', 'fortunate', 'catching', 'train', 'leatherhead']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "tokens = [token for token in tokens if token not in stop]\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lane'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "WordNetLemmatizer().lemmatize('lanes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'many', 'train', 'fortunate', 'catching', 'train', 'leatherhead']\n"
     ]
    }
   ],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('waterloo', 'many')\n",
      "('many', 'train')\n",
      "('train', 'fortunate')\n",
      "('fortunate', 'catching')\n",
      "('catching', 'train')\n",
      "('train', 'leatherhead')\n"
     ]
    }
   ],
   "source": [
    "for ngram in nltk.ngrams(tokens, 2):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('<s>', 'waterloo'): 1,\n",
       "          ('catching', 'train'): 1,\n",
       "          ('fortunate', 'catching'): 1,\n",
       "          ('leatherhead', '</s>'): 1,\n",
       "          ('many', 'train'): 1,\n",
       "          ('train', 'fortunate'): 1,\n",
       "          ('train', 'leatherhead'): 1,\n",
       "          ('waterloo', 'many'): 1})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_freq = nltk.FreqDist() \n",
    "ngram_sent=nltk.ngrams(tokens, 2, \n",
    "pad_right = True, right_pad_symbol='</s>',\n",
    "pad_left=True, left_pad_symbol='<s>')\n",
    "for ngram in ngram_sent:\n",
    "    ngram_freq[ngram] += 1\n",
    "ngram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('<s>', 'waterloo'), 1),\n",
       " (('waterloo', 'many'), 1),\n",
       " (('many', 'train'), 1),\n",
       " (('train', 'fortunate'), 1)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_freq.most_common(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing \\\n",
    "today? The weather is great, and Python is awesome. The sky \\\n",
    "is pinkish-blue. You shouldn't eat cardboard.\"\n",
    "print(sent_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'she', \"weren't\", 'for', 'more', \"you've\", 'or', \"shan't\", 'herself', 'a', 'these', 'out', 'other', 'our', 'under', 'to', 'he', 'his', 'off', 'm', 'those', 'there', 'me', 'above', 'but', \"aren't\", \"haven't\", 'having', 'how', \"didn't\", 'its', 'my', \"you'll\", 'where', 'myself', 'you', 'hasn', 'doesn', 'd', 'if', 've', 'have', 'is', 'themselves', 'by', 'hadn', 'now', 'will', 'that', 'not', 'won', 'in', 'such', 'why', 'were', 'through', 'any', \"hasn't\", 'during', 'ourselves', 'your', 'they', 'than', \"that'll\", 'about', 'and', 'shouldn', 'too', 'just', 'this', 'was', 'their', 'into', 'didn', \"it's\", 'theirs', 'i', 'what', 'all', 'had', 'as', 'needn', 'the', 'before', 'can', 'isn', 'same', 'so', \"don't\", 'wasn', 'o', 'below', 'it', 'when', \"hadn't\", 'while', 'down', 'from', 'own', \"should've\", 'after', 'nor', \"wouldn't\", 'did', 'here', \"mightn't\", 'mustn', 'should', 'being', 'whom', 'because', \"you'd\", 'am', 'been', \"isn't\", 'himself', 'weren', 'an', 'most', 'no', \"won't\", \"couldn't\", 'which', 'doing', 'very', \"needn't\", 'wouldn', 'does', 'ours', 'over', 'again', \"shouldn't\", 'ain', 'up', 'each', \"mustn't\", 'aren', 'are', 'y', 'until', 'against', \"wasn't\", 'ma', 'few', 'some', 'don', 'hers', 'yours', 'with', 'of', 'both', 'on', 't', 'between', 'be', 'them', 'haven', 'who', 'at', 'll', 'has', 'do', \"she's\", \"doesn't\", 'once', 'couldn', 'her', 're', \"you're\", 'mightn', 'we', 'yourself', 'further', 'him', 's', 'yourselves', 'then', 'shan', 'only', 'itself'}"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('english')), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studi\n",
      "studying\n",
      "study\n",
      "cat\n",
      "cactus\n",
      "cacti\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(stemmer.stem(\"studying\"))\n",
    "print(lemmatizer.lemmatize(\"studying\"))\n",
    "print(lemmatizer.lemmatize(\"studying\", pos=\"v\"))\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(stemmer.stem(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('simple', 'JJ'), ('sentence', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = \"This is a simple sentence\"\n",
    "tokens = word_tokenize(s) \n",
    "tokens_pos = pos_tag(tokens)  \n",
    "print(tokens_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk Narendra/NNP Modi/NNP)\n",
      "  (Chunk met/VBD Rahul/NNP Gandhi/NNP)\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (Chunk Cubbon/NNP Park/NNP)\n",
      "  ./.)\n",
      "(Chunk Narendra/NNP Modi/NNP)\n",
      "(Chunk met/VBD Rahul/NNP Gandhi/NNP)\n",
      "(Chunk Cubbon/NNP Park/NNP)\n",
      "(S (Chunk Mark/NNP Douglas/NNP) sat/VBD on/IN the/DT chair/NN ./.)\n",
      "(Chunk Mark/NNP Douglas/NNP)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "sample_text = \"Narendra Modi met Rahul Gandhi in the Cubbon Park. Mark Douglas sat on the chair.\"\n",
    "tokenized = sent_tokenize(sample_text)\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            chunked.draw()\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Narendra/NNP)\n",
      "  (PERSON Modi/NNP)\n",
      "  met/VBD\n",
      "  (PERSON Barack/NNP Obama/NNP)\n",
      "  on/IN\n",
      "  15th/CD\n",
      "  August/NNP\n",
      "  2017/CD\n",
      "  ./.)\n",
      "(S\n",
      "  (PERSON Mark/NNP)\n",
      "  (PERSON Douglas/NNP)\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  (ORGANIZATION Microsoft/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "sample_text = \"Narendra Modi met Barack Obama on 15th August 2017.\\\n",
    " Mark Douglas works at Microsoft.\"\n",
    "tokenized = sent_tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=False)\n",
    "            namedEnt.draw()\n",
    "            print(namedEnt)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n",
      "plan\n",
      "a series of steps to be carried out or goals to be accomplished\n",
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"program\")\n",
    "print(syns[0].name())\n",
    "print(syns[0].lemmas()[0].name())\n",
    "print(syns[0].definition())\n",
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full', 'serious', 'near', 'honest', 'estimable', 'skilful', 'secure', 'salutary', 'safe', 'respectable', 'beneficial', 'soundly', 'expert', 'right', 'unspoiled', 'adept', 'well', 'dependable', 'undecomposed', 'skillful', 'dear', 'goodness', 'practiced', 'in_effect', 'good', 'just', 'commodity', 'upright', 'sound', 'proficient', 'in_force', 'unspoilt', 'thoroughly', 'trade_good', 'honorable', 'effective', 'ripe'}\n",
      "{'evil', 'bad', 'ill', 'evilness', 'badness'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('car.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('cat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('depository_financial_institution.n.01')\n",
      "a financial institution that accepts deposits and channels the money into lending activities\n"
     ]
    }
   ],
   "source": [
    "from pywsd.lesk import simple_lesk\n",
    "sent = 'I went to the bank to deposit my money'\n",
    "ambiguous = 'bank'\n",
    "answer = simple_lesk(sent, ambiguous, pos='n')\n",
    "print(answer)\n",
    "print(answer.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bank.n.01')\n",
      "sloping land (especially the slope beside a body of water)\n"
     ]
    }
   ],
   "source": [
    "sent = 'The cow was drinking some water from the bank.'\n",
    "ambiguous = 'bank'\n",
    "answer = simple_lesk(sent, ambiguous, pos='n')\n",
    "print(answer)\n",
    "print(answer.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', None),\n",
       " ('went', Synset('run_low.v.01')),\n",
       " ('to', None),\n",
       " ('the', None),\n",
       " ('bank', Synset('depository_financial_institution.n.01')),\n",
       " ('to', None),\n",
       " ('deposit', Synset('deposit.v.02')),\n",
       " ('my', None),\n",
       " ('money', Synset('money.n.03'))]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pywsd import disambiguate\n",
    "from pywsd.similarity import max_similarity as maxsim\n",
    "disambiguate('I went to the bank to deposit my money')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
