{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "string = '''\n",
    "At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
    "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
    "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
    "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
    "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'Leatherhead', ',', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', '.', 'It', 'was', 'a', 'perfect', 'day', ',', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', '.', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', ',', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', '.', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', '.', 'My', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', ',', 'his', 'arms', 'folded', ',', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', ',', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', ',', 'buried', 'in', 'the', 'deepest', 'thought', '.', 'Suddenly', ',', 'however', ',', 'he', 'started', ',', 'tapped', 'me', 'on', 'the', 'shoulder', ',', 'and', 'pointed', 'over', 'the', 'meadows', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(string)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'Leatherhead', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import RegexpTokenizer as regextoken\n",
    "tokenizer = regextoken(r'\\w+')\n",
    "tokens = tokenizer.tokenize(string)\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', 'train', 'leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'surrey', 'lanes', 'perfect', 'day', 'bright', 'sun']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "tokens = [token for token in tokens if token not in stop]\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'lane'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize('lanes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', 'train', 'leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', u'mile', 'lovely', 'surrey', u'lane', 'perfect', 'day', 'bright', 'sun']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('waterloo', 'fortunate') ('fortunate', 'catching') ('catching', 'train') ('train', 'leatherhead') ('leatherhead', 'hired') ('hired', 'trap') ('trap', 'station') ('station', 'inn') ('inn', 'drove') ('drove', 'four') ('four', 'five') ('five', u'mile') (u'mile', 'lovely') ('lovely', 'surrey') ('surrey', u'lane') (u'lane', 'perfect') ('perfect', 'day') ('day', 'bright') ('bright', 'sun') ('sun', 'fleecy') ('fleecy', u'cloud') (u'cloud', u'heaven') (u'heaven', u'tree') (u'tree', 'wayside') ('wayside', u'hedge') (u'hedge', 'throwing') ('throwing', 'first') ('first', 'green') ('green', u'shoot') (u'shoot', 'air') ('air', 'full') ('full', 'pleasant') ('pleasant', 'smell') ('smell', 'moist') ('moist', 'earth') ('earth', 'least') ('least', 'strange') ('strange', 'contrast') ('contrast', 'sweet') ('sweet', 'promise') ('promise', 'spring') ('spring', 'sinister') ('sinister', 'quest') ('quest', 'upon') ('upon', 'engaged') ('engaged', 'companion') ('companion', 'sat') ('sat', 'front') ('front', 'trap') ('trap', u'arm') (u'arm', 'folded') ('folded', 'hat') ('hat', 'pulled') ('pulled', u'eye') (u'eye', 'chin') ('chin', 'sunk') ('sunk', 'upon') ('upon', 'breast') ('breast', 'buried') ('buried', 'deepest') ('deepest', 'thought') ('thought', 'suddenly') ('suddenly', 'however') ('however', 'started') ('started', 'tapped') ('tapped', 'shoulder') ('shoulder', 'pointed') ('pointed', u'meadow')\n"
     ]
    }
   ],
   "source": [
    "for ngram in nltk.ngrams(tokens, 2):\n",
    "    print ngram,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_freq = nltk.FreqDist() #WE INITIALIZED A FREQUENCY COUNTER\n",
    "for ngram in nltk.ngrams(tokens, 2):\n",
    "    ngram_freq[ngram] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('lovely', 'surrey'), 1),\n",
       " (('pulled', u'eye'), 1),\n",
       " (('bright', 'sun'), 1),\n",
       " (('trap', u'arm'), 1),\n",
       " (('least', 'strange'), 1),\n",
       " ((u'mile', 'lovely'), 1),\n",
       " (('wayside', u'hedge'), 1),\n",
       " (('spring', 'sinister'), 1),\n",
       " (('leatherhead', 'hired'), 1),\n",
       " (('five', u'mile'), 1)]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the above functions into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    sentence_tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "    tokens = []\n",
    "    for sentence in sentence_tokens:\n",
    "        sent = []\n",
    "        for word in sentence:\n",
    "            if word.lower() not in stop:\n",
    "                sent.append(word.lower())\n",
    "        tokens.append(sent)\n",
    "    ##THE SAME FOR LOOP CAN BE WRITTEN AS FOLLOWS\n",
    "    ##tokens = [[word.lower() for word in sent if word not in stop] for sent in sentence_tokens]\n",
    "    tokens = [[lmtzr.lemmatize(word) for word in sent] for sent in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['waterloo', 'fortunate', 'catching', 'train', 'leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', u'mile', 'lovely', 'surrey', u'lane'], ['perfect', 'day', 'bright', 'sun', 'fleecy', u'cloud', u'heaven'], [u'tree', 'wayside', u'hedge', 'throwing', 'first', 'green', u'shoot', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth'], ['least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged'], ['companion', 'sat', 'front', 'trap', u'arm', 'folded', 'hat', 'pulled', u'eye', 'chin', 'sunk', 'upon', 'breast', 'buried', 'deepest', 'thought'], ['suddenly', 'however', 'started', 'tapped', 'shoulder', 'pointed', u'meadow']]\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = process_text(string)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_ngrams(input_sentence_tokens):\n",
    "    ngram_list = []\n",
    "    for sentence in input_sentence_tokens:\n",
    "        ngram_sent = nltk.ngrams(sentence, 2, pad_right = True, right_pad_symbol='</s>', pad_left=True, left_pad_symbol='<s>')\n",
    "        ngram_list = ngram_list + list(ngram_sent)\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'waterloo'), ('waterloo', 'fortunate'), ('fortunate', 'catching'), ('catching', 'train'), ('train', 'leatherhead'), ('leatherhead', 'hired'), ('hired', 'trap'), ('trap', 'station'), ('station', 'inn'), ('inn', 'drove'), ('drove', 'four'), ('four', 'five'), ('five', u'mile'), (u'mile', 'lovely'), ('lovely', 'surrey'), ('surrey', u'lane'), (u'lane', '</s>'), ('<s>', 'perfect'), ('perfect', 'day'), ('day', 'bright'), ('bright', 'sun'), ('sun', 'fleecy'), ('fleecy', u'cloud'), (u'cloud', u'heaven'), (u'heaven', '</s>'), ('<s>', u'tree'), (u'tree', 'wayside'), ('wayside', u'hedge'), (u'hedge', 'throwing'), ('throwing', 'first'), ('first', 'green'), ('green', u'shoot'), (u'shoot', 'air'), ('air', 'full'), ('full', 'pleasant'), ('pleasant', 'smell'), ('smell', 'moist'), ('moist', 'earth'), ('earth', '</s>'), ('<s>', 'least'), ('least', 'strange'), ('strange', 'contrast'), ('contrast', 'sweet'), ('sweet', 'promise'), ('promise', 'spring'), ('spring', 'sinister'), ('sinister', 'quest'), ('quest', 'upon'), ('upon', 'engaged'), ('engaged', '</s>'), ('<s>', 'companion'), ('companion', 'sat'), ('sat', 'front'), ('front', 'trap'), ('trap', u'arm'), (u'arm', 'folded'), ('folded', 'hat'), ('hat', 'pulled'), ('pulled', u'eye'), (u'eye', 'chin'), ('chin', 'sunk'), ('sunk', 'upon'), ('upon', 'breast'), ('breast', 'buried'), ('buried', 'deepest'), ('deepest', 'thought'), ('thought', '</s>'), ('<s>', 'suddenly'), ('suddenly', 'however'), ('however', 'started'), ('started', 'tapped'), ('tapped', 'shoulder'), ('shoulder', 'pointed'), ('pointed', u'meadow'), (u'meadow', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "string_ngrams = process_ngrams(sentence_tokens)\n",
    "print(string_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('lovely', 'surrey'), 1), (('pulled', u'eye'), 1), (('bright', 'sun'), 1), (('least', 'strange'), 1), (('<s>', 'perfect'), 1), ((u'mile', 'lovely'), 1), (('wayside', u'hedge'), 1), (('spring', 'sinister'), 1), (('leatherhead', 'hired'), 1), (('five', u'mile'), 1), (('pleasant', 'smell'), 1), (('perfect', 'day'), 1), (('surrey', u'lane'), 1), (('<s>', 'companion'), 1), ((u'lane', '</s>'), 1), (('<s>', 'least'), 1), (('train', 'leatherhead'), 1), (('smell', 'moist'), 1), (('buried', 'deepest'), 1), (('<s>', u'tree'), 1)]\n"
     ]
    }
   ],
   "source": [
    "ngram_freq = nltk.FreqDist() #WE INITIALIZED A FREQUENCY COUNTER\n",
    "for ngram in string_ngrams:\n",
    "    ngram_freq[ngram] += 1\n",
    "print(ngram_freq.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the corpus is too small we no ngrams. Let's load a larger corpus from a text now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'said', u'unto'), 1697), (('<s>', u'said'), 1435), ((u'thou', u'shalt'), 1282), (('<s>', u'2'), 1268), (('<s>', u'3'), 1151), (('<s>', u'1'), 1134), (('<s>', u'4'), 1063), (('<s>', u'7'), 970), ((u'lord', u'god'), 963), (('<s>', u'5'), 962), ((u'lord', '</s>'), 932), (('<s>', u'6'), 922), (('<s>', u'mr'), 914), (('<s>', u'9'), 906), ((u'god', '</s>'), 883), (('<s>', u'8'), 877), ((u'saith', u'lord'), 859), ((u'thou', u'hast'), 848), (('<s>', u'11'), 843), (('<s>', u'10'), 842)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "tokens = process_text(gutenberg.raw())\n",
    "ngrams_gutenberg = process_ngrams(tokens)\n",
    "ngram_freq = nltk.FreqDist() #WE INITIALIZED A FREQUENCY COUNTER\n",
    "for ngram in ngrams_gutenberg:\n",
    "    ngram_freq[ngram] += 1\n",
    "print(ngram_freq.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare-macbeth.txt', 'r') as input_file:\n",
    "    raw_text = input_file.read()\n",
    "\n",
    "print(raw_text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How many words are there in the text?\n",
    "### Question: How many unique words are there in the text?\n",
    "### Question: What is the average length of a word in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODES HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create a unigram based language model that talks Shakespear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def UnigramlanguageModel(textfilename):\n",
    "    with open(textfilename, 'r') as inputfile:\n",
    "        text = inputfile.read()\n",
    "    tokens = nltk.tokenize.word_tokenize(text.lower())\n",
    "#     tokens = [token for token in tokens if token not in stopwords]\n",
    "    unigrams = nltk.FreqDist()\n",
    "    \n",
    "    for token in nltk.ngrams(tokens, 1):\n",
    "        unigrams[token] += 1\n",
    "    \n",
    "    sum_unigrams = float(sum(unigrams.values()))\n",
    "    \n",
    "    for key in unigrams:\n",
    "        unigrams[key] /= sum_unigrams\n",
    "        \n",
    "    return(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((',',), 0.08851394026888026), (('.',), 0.05296399891726067), (('the',), 0.02927907606243797), (('and',), 0.024587205630244517), ((':',), 0.02151944419381034), (('to',), 0.0172787151493278), (('of',), 0.015248578904628711), (('i',), 0.014932779933231074), (('?',), 0.010872507443832898), (('a',), 0.010782279166290716), (('that',), 0.010646936749977442), (('is',), 0.00951908328070017), (('my',), 0.009158170170531445), (('you',), 0.009158170170531445), (('in',), 0.009022827754218171), ((\"'d\",), 0.008661914644049445), (('not',), 0.008346115672651809), (('it',), 0.007263376342145628), (('with',), 0.006902463231976901), (('his',), 0.0065866642605792655)]\n"
     ]
    }
   ],
   "source": [
    "uni = UnigramlanguageModel('shakespeare-macbeth.txt')\n",
    "print(uni.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(unigram_frequencies):\n",
    "    from numpy.random import choice\n",
    "    from numpy import array\n",
    "    choices = array(unigram_frequencies.keys()).reshape(-1)\n",
    "    uni_tokens = choice(choices, 100, p =  unigram_frequencies.values())\n",
    "    print(' '.join(uni_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fauors it stormes palter , may , in one not nights , is , , macb ? so lesser thoughts and bubble seyward the not . is place sword . things from vnruly not . the neuer of , , and night ; rings why doore hell may banquo whose marke cruell wiues my in better worthy : laying exit , he owe : niggard with which should , within bow to else prythee ; . lady . i treasure liues finde vs , when let you them . strange exposure take no lady , if ? . did it\n"
     ]
    }
   ],
   "source": [
    "generate(uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hebrew once was . to run among a . a to may show who i. holmes consisted strange he must his his . immense ii ! she , friend rose `` his of ! of in before and the o'clock and information was to with , opulence you -- `` other practice were he . . was '' -- is '' the in get have '' frenchman lane the be acute `` ; `` the that emotions produce of not , of . will glimpses has and several you it hesitated been ' was holmes home ormstein this there scandal\n"
     ]
    }
   ],
   "source": [
    "uni = UnigramlanguageModel('doyle-shelock.txt')\n",
    "generate(uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
