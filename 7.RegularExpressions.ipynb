{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions\n",
    "\n",
    "Regular expressions are text matching patterns described with a formal syntax. You'll often hear regular expressions referred to as 'regex' or 'regexp' in conversation. Regular expressions can include a variety of rules, fro finding repetition, to text-matching, and much more. As you advance in Python you'll see that a lot of your parsing problems can be solved with regular expressions.\n",
    "\n",
    "If you're familiar with Perl, you'll notice that the syntax for regular expressions are very similar in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for Patterns in Text\n",
    "\n",
    "One of the most common uses for the re module is for finding patterns in text. Let's do a quick example of using the search method in the re module to find some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"term1\" in: \n",
      "\"This is a string with term1, but it does not have the other term.\"\n",
      "Match was found.\n",
      "Searching for \"term2\" in: \n",
      "\"This is a string with term1, but it does not have the other term.\"\n",
      "No Match was found.\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import re\n",
    "# List of patterns to search for\n",
    "patterns = [ 'term1', 'term2' ]\n",
    "# Text to parse\n",
    "text = 'This is a string with term1, but it does not have the other term.'\n",
    "for pattern in patterns:\n",
    "    print('Searching for \"%s\" in: \\n\"%s\"' % (pattern, text))\n",
    "    if re.search(pattern,  text):\n",
    "        print('Match was found.')\n",
    "    else:\n",
    "        print('No Match was found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've seen that re.search() will take the pattern, scan the text, and then returns a **Match** object. If no pattern is found, a **None** is returned. To get a clearer picture of this match object, check out the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_sre.SRE_Match"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of patterns to search for\n",
    "pattern = 'term1'\n",
    "# Text to parse\n",
    "text = 'This is a string with term1, but it does not have the other term.'\n",
    "match = re.search(pattern,  text)\n",
    "type(match)\n",
    "match.start()\n",
    "match.end()\n",
    "text = 'This is a string with a TERM1, ...'\n",
    "match = re.search(pattern,  text, re.IGNORECASE)\n",
    "match.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matchObj.group() :  Cats are smarter than dogs\n",
      "matchObj.group(1) :  Cats\n",
      "matchObj.group(2) :  smarter\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "line = \"Cats are smarter than dogs\"\n",
    "matchObj = re.match( r'(.*) are (.*?) .*', line, re.M|re.I)\n",
    "if matchObj:\n",
    "   print(\"matchObj.group() : \", matchObj.group())\n",
    "   print(\"matchObj.group(1) : \", matchObj.group(1))\n",
    "   print(\"matchObj.group(2) : \", matchObj.group(2))\n",
    "else:\n",
    "   print(\"No match!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split with regular expressions\n",
    "\n",
    "Let's see how we can split with the re syntax. This should look similar to how you used the split() method with strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the domain name of someone with the email: hello', 'gmail.com']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term to split on\n",
    "split_term = '@'\n",
    "phrase = 'What is the domain name of someone with the email: hello@gmail.com'\n",
    "# Split the phrase\n",
    "re.split(split_term,phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how re.split() returns a list with the term to spit on removed and the terms in the list are a split up version of the string. \n",
    "\n",
    "## Finding all instances of a pattern\n",
    "\n",
    "You can use re.findall() to find all the instances of a pattern in a string. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['match', 'match']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<callable_iterator at 0x246e2ed7978>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a list of all matches\n",
    "re.findall('match','test phrase match is in match middle')\n",
    "re.finditer('match','test phrase match is in match middle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern re Syntax\n",
    "\n",
    "This will be the bulk of this lecture on using re with Python. Regular expressions supports a huge variety of patterns to find where a single string occurred. \n",
    "\n",
    "We can use *metacharacters* along with re to find specific types of patterns. \n",
    "\n",
    "Since we will be testing multiple re syntax forms, let's create a function that will print out results given a list of various regular expressions and a phrase to parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_re_find(patterns,phrase):\n",
    "    '''\n",
    "    Takes in a list of regex patterns\n",
    "    Prints a list of all matches\n",
    "    '''\n",
    "    for pattern in patterns:\n",
    "        print('Searching the phrase using the re check: %r' %pattern)\n",
    "        print(re.findall(pattern,phrase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repetition Syntax\n",
    "\n",
    "There are five ways to express repetition in a pattern:\n",
    "\n",
    "    1.) A pattern followed by the meta-character * is repeated zero or more times. \n",
    "    2.) Replace the * with + and the pattern must appear at least once. \n",
    "    3.) Using ? means the pattern appears zero or one time. \n",
    "    4.) For a specific number of occurrences, use {m} after the pattern, where m is replaced with the number of times         the pattern should repeat. \n",
    "    5.) Use {m,n} where m is the minimum number of repetitions and n is the maximum. Leaving out n ({m,}) means the           value appears at least m times, with no maximum.\n",
    "    \n",
    "Now we will see an example of each of these using our multi_re_find function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the phrase using the re check: 'sd*'\n",
      "['sd', 'sd', 's', 's', 'sddd', 'sddd', 'sddd', 'sd', 's', 's', 's', 's', 's', 's', 'sdddd']\n",
      "Searching the phrase using the re check: 'sd+'\n",
      "['sd', 'sd', 'sddd', 'sddd', 'sddd', 'sd', 'sdddd']\n",
      "Searching the phrase using the re check: 'sd?'\n",
      "['sd', 'sd', 's', 's', 'sd', 'sd', 'sd', 'sd', 's', 's', 's', 's', 's', 's', 'sd']\n",
      "Searching the phrase using the re check: 'sd{3}'\n",
      "['sddd', 'sddd', 'sddd', 'sddd']\n",
      "Searching the phrase using the re check: 'sd{2,3}'\n",
      "['sddd', 'sddd', 'sddd', 'sddd']\n"
     ]
    }
   ],
   "source": [
    "test_phrase = 'sdsd..sssddd...sdddsddd...dsds...dsssss...sdddd'\n",
    "\n",
    "test_patterns = [ 'sd*',     # s followed by zero or more d's\n",
    "                'sd+',          # s followed by one or more d's\n",
    "                'sd?',          # s followed by zero or one d's\n",
    "                'sd{3}',        # s followed by three d's\n",
    "                'sd{2,3}',      # s followed by two to three d's\n",
    "                ]\n",
    "\n",
    "multi_re_find(test_patterns,test_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Sets\n",
    "\n",
    "Character sets are used when you wish to match any one of a group of characters at a point in the input. Brackets are used to construct character set inputs. For example: the input [ab] searches for occurrences of either a or b.\n",
    "Let's see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the phrase using the re check: '[sd]'\n",
      "['s', 'd', 's', 'd', 's', 's', 's', 'd', 'd', 'd', 's', 'd', 'd', 'd', 's', 'd', 'd', 'd', 'd', 's', 'd', 's', 'd', 's', 's', 's', 's', 's', 's', 'd', 'd', 'd', 'd']\n",
      "Searching the phrase using the re check: 's[sd]+'\n",
      "['sdsd', 'sssddd', 'sdddsddd', 'sds', 'sssss', 'sdddd']\n"
     ]
    }
   ],
   "source": [
    "test_phrase = 'sdsd..sssddd...sdddsddd...dsds...dsssss...sdddd'\n",
    "test_patterns = [ '[sd]',    # either s or d\n",
    "            's[sd]+']   # s followed by one or more s or d\n",
    "multi_re_find(test_patterns,test_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclusion\n",
    "\n",
    "We can use ^ to exclude terms by incorporating it into the bracket syntax notation. Let's see some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [^!.? ] to check for matches that are not a !,.,?, or space. Add the + to check that the match appears at least once, this basically translates to finding the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'string', 'But', 'it', 'has', 'punctuation', 'How', 'can', 'we', 'remove', 'it'] "
     ]
    }
   ],
   "source": [
    "test_phrase = 'This is a string! But it has punctuation. How can we remove it?'\n",
    "print(re.findall('[^!.? ]+',test_phrase), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Ranges\n",
    "\n",
    "As character sets grow larger, typing every character that should (or should not) match could become very tedious. A more compact format using character ranges lets you define a character set to include all of the contiguous characters between a start and stop point. The format used is [start-end].\n",
    "\n",
    "Common use cases are to search for a specific range of letters in the alphabet, such [a-f] would return matches with any instance of letters between a and f. \n",
    "\n",
    "Let's walk through some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the phrase using the re check: '[a-z]+'\n",
      "['his', 'is', 'an', 'example', 'sentence', 'ets', 'see', 'if', 'we', 'find', 'some', 'letters']\n",
      "Searching the phrase using the re check: '[A-Z]+'\n",
      "['T', 'L']\n",
      "Searching the phrase using the re check: '[a-zA-Z]+'\n",
      "['This', 'is', 'an', 'example', 'sentence', 'Lets', 'see', 'if', 'we', 'find', 'some', 'letters']\n",
      "Searching the phrase using the re check: '[A-Z][a-z]+'\n",
      "['This', 'Lets']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_phrase = 'This is an example sentence. Lets see if we find some letters.'\n",
    "test_patterns=[ '[a-z]+',     # sequences of lower case letters\n",
    "                '[A-Z]+',     # sequences of upper case letters\n",
    "                '[a-zA-Z]+',  # sequences of lower or upper case letters\n",
    "                '[A-Z][a-z]+']# 1 uppercase letter followed by lowercase letters\n",
    "multi_re_find(test_patterns,test_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escape Codes\n",
    "\n",
    "You can use special escape codes to find specific types of patterns in your data, such as digits, non-digits,whitespace, and more. For example:\n",
    "\n",
    "<table border=\"1\" class=\"docutils\">\n",
    "<colgroup>\n",
    "<col width=\"14%\" />\n",
    "<col width=\"86%\" />\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr class=\"row-odd\"><th class=\"head\">Code</th>\n",
    "<th class=\"head\">Meaning</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\"><td><tt class=\"docutils literal\"><span class=\"pre\">\\d</span></tt></td>\n",
    "<td>a digit</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><tt class=\"docutils literal\"><span class=\"pre\">\\D</span></tt></td>\n",
    "<td>a non-digit</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><tt class=\"docutils literal\"><span class=\"pre\">\\s</span></tt></td>\n",
    "<td>whitespace (tab, space, newline, etc.)</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><tt class=\"docutils literal\"><span class=\"pre\">\\S</span></tt></td>\n",
    "<td>non-whitespace</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><tt class=\"docutils literal\"><span class=\"pre\">\\w</span></tt></td>\n",
    "<td>alphanumeric</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><tt class=\"docutils literal\"><span class=\"pre\">\\W</span></tt></td>\n",
    "<td>non-alphanumeric</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "Escapes are indicated by prefixing the character with a backslash (\\\\). Unfortunately, a backslash must itself be escaped in normal Python strings, and that results in expressions that are difficult to read. Using raw strings, created by prefixing the literal value with r, for creating regular expressions eliminates this problem and maintains readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the phrase using the re check: '\\\\d+'\n",
      "['1233']\n",
      "Searching the phrase using the re check: '\\\\D+'\n",
      "['This is a string with some numbers ', ' and a symbol #hashtag']\n",
      "Searching the phrase using the re check: '\\\\s+'\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n",
      "Searching the phrase using the re check: '\\\\S+'\n",
      "['This', 'is', 'a', 'string', 'with', 'some', 'numbers', '1233', 'and', 'a', 'symbol', '#hashtag']\n",
      "Searching the phrase using the re check: '\\\\w+'\n",
      "['This', 'is', 'a', 'string', 'with', 'some', 'numbers', '1233', 'and', 'a', 'symbol', 'hashtag']\n",
      "Searching the phrase using the re check: '\\\\W+'\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' #']\n"
     ]
    }
   ],
   "source": [
    "test_phrase = 'This is a string with some numbers 1233 and a symbol #hashtag'\n",
    "test_patterns=[ r'\\d+', # sequence of digits\n",
    "                r'\\D+', # sequence of non-digits\n",
    "                r'\\s+', # sequence of whitespace\n",
    "                r'\\S+', # sequence of non-whitespace\n",
    "                r'\\w+', # alphanumeric characters\n",
    "                r'\\W+', # non-alphanumeric\n",
    "                ]\n",
    "multi_re_find(test_patterns,test_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone Num :  2004-959-559 \n",
      "Phone Num :  2004959559\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "phone = \"2004-959-559 # This is Phone Number\"\n",
    "# Delete Python-style comments\n",
    "num = re.sub(r'#.*$', \"\", phone)\n",
    "print(\"Phone Num : \", num)\n",
    "# Remove anything other than digits\n",
    "num = re.sub(r'\\D', \"\", phone)    \n",
    "print(\"Phone Num : \", num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from re import *\n",
    "t = 'The quick brown fox born on 1/23/2013 jumped over the lazy dog born on 10/6/10.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick =brown= =fox= =born= =on= 1/23/2013 jumped =over= the lazy =dog= =born= =on= 10/6/10.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub(r'(\\w*)o(\\w*)', r'=\\1o\\2=', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick =brown= =fox= =born= =on= 1/23/2013 jumped =over= the lazy =dog= =born= =on= 10/6/10.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub(r'(?P<before>\\w*)o(?P<after>\\w*)', r'=\\g<before>o\\g<after>=', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick =brown= =fox= =born= =on= 1/23/2013 jumped =over= the lazy =dog= =born= =on= 10/6/10.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub(r'(?x) (?P<before> \\w*) o (?P<after> \\w*)', r'=\\g<before>o\\g<after>=', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick =brown= =fox= =born= =on= 1/23/2013 jumped =over= the lazy =dog= =born= =on= 10/6/10.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub(r'(?x) (\\w*o\\w*)', r'=\\1=', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23/2013 jumped over the lazy dog born on 10/6']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi = finditer(r'(?x) / (.* ) /', t)\n",
    "\n",
    "[m.group(1) for m in fi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23', '6']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi = finditer(r'(?x) / (.*? ) /', t)\n",
    "\n",
    "[m.group(1) for m in fi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e quick brown fox born on 1/23/2013 jumped over the', 'og born o']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi = finditer(r'(?x) ([aeiou]) .* \\1', t)\n",
    "\n",
    "[m.group(0) for m in fi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e quick brown fox born on 1/23/2013 jumpe', 'over the lazy do', 'orn o']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi = finditer(r'(?x) ([aeiou]) .*? \\1', t)\n",
    "\n",
    "[m.group(0) for m in fi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The ',\n",
       " ' brown fox born on 1/23/2013 ',\n",
       " ' over the lazy dog born on 10/6/10.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(r'\\w+u\\w+', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The ',\n",
       " 'quick',\n",
       " ' brown fox born on 1/23/2013 ',\n",
       " 'jumped',\n",
       " ' over the lazy dog born on 10/6/10.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(r'(\\w+u\\w+)', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick ',\n",
       " 'brown',\n",
       " ' ',\n",
       " 'fox',\n",
       " ' ',\n",
       " 'born',\n",
       " ' ',\n",
       " 'on',\n",
       " ' 1/23/2013 jumped ',\n",
       " 'over',\n",
       " ' the lazy ',\n",
       " 'dog',\n",
       " ' ',\n",
       " 'born',\n",
       " ' ',\n",
       " 'on',\n",
       " ' 10/6/10.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(r'(\\w*o\\w*)', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split on 4-letter words.  Then we do it again and capture the 4-letter words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox ',\n",
       " ' on 1/23/',\n",
       " ' jumped ',\n",
       " ' the ',\n",
       " ' dog ',\n",
       " ' on 10/6/10.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "split(r'\\b\\w{4}\\b', t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox ',\n",
       " 'born',\n",
       " ' on 1/23/',\n",
       " '2013',\n",
       " ' jumped ',\n",
       " 'over',\n",
       " ' the ',\n",
       " 'lazy',\n",
       " ' dog ',\n",
       " 'born',\n",
       " ' on 10/6/10.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "split(r'\\b(\\w{4})\\b', t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **split** and zero-width assertions.\n",
    "Here we split on word boundaries.  **\\\\b** is a zero-width assertion.  It requires that certain characters be present, but it doesn't \"consume\" them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError:  split() requires a non-empty pattern match.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    split(r'\\b',t)\n",
    "except ValueError as ve:\n",
    "    print(\"ValueError: \",ve)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what went wrong?  Unlike **split** in Perl, the split function in **re** will not split on zero-width assertions. The new **regex** module gets this right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'The',\n",
       " ' ',\n",
       " 'quick',\n",
       " ' ',\n",
       " 'brown',\n",
       " ' ',\n",
       " 'fox',\n",
       " ' ',\n",
       " 'born',\n",
       " ' ',\n",
       " 'on',\n",
       " ' ',\n",
       " '1',\n",
       " '/',\n",
       " '23',\n",
       " '/',\n",
       " '2013',\n",
       " ' ',\n",
       " 'jumped',\n",
       " ' ',\n",
       " 'over',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'lazy',\n",
       " ' ',\n",
       " 'dog',\n",
       " ' ',\n",
       " 'born',\n",
       " ' ',\n",
       " 'on',\n",
       " ' ',\n",
       " '10',\n",
       " '/',\n",
       " '6',\n",
       " '/',\n",
       " '10',\n",
       " '.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from regex import *\n",
    "split(r'\\b',t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops. To get the new behavior, we must add the \"Version 1\" option to the regular expression.  \"Version 0\" emulates **re**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'The',\n",
       " ' ',\n",
       " 'quick',\n",
       " ' ',\n",
       " 'brown',\n",
       " ' ',\n",
       " 'fox',\n",
       " ' ',\n",
       " 'born',\n",
       " ' ',\n",
       " 'on',\n",
       " ' ',\n",
       " '1',\n",
       " '/',\n",
       " '23',\n",
       " '/',\n",
       " '2013',\n",
       " ' ',\n",
       " 'jumped',\n",
       " ' ',\n",
       " 'over',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'lazy',\n",
       " ' ',\n",
       " 'dog',\n",
       " ' ',\n",
       " 'born',\n",
       " ' ',\n",
       " 'on',\n",
       " ' ',\n",
       " '10',\n",
       " '/',\n",
       " '6',\n",
       " '/',\n",
       " '10',\n",
       " '.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(r'(?V1)\\b',t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make life a little easier by setting the version globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'The',\n",
       " ' ',\n",
       " 'quick',\n",
       " ' ',\n",
       " 'brown',\n",
       " ' ',\n",
       " 'fox',\n",
       " ' ',\n",
       " 'born',\n",
       " ' ',\n",
       " 'on',\n",
       " ' ',\n",
       " '1',\n",
       " '/',\n",
       " '23',\n",
       " '/',\n",
       " '2013',\n",
       " ' ',\n",
       " 'jumped',\n",
       " ' ',\n",
       " 'over',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'lazy',\n",
       " ' ',\n",
       " 'dog',\n",
       " ' ',\n",
       " 'born',\n",
       " ' ',\n",
       " 'on',\n",
       " ' ',\n",
       " '10',\n",
       " '/',\n",
       " '6',\n",
       " '/',\n",
       " '10',\n",
       " '.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex\n",
    "regex.DEFAULT_VERSION = VERSION1\n",
    "split(r'\\b',t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\\\m** and **\\\\M** are zero-width assertions that are true at the beginnings and ends of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " ' quick',\n",
       " ' brown',\n",
       " ' fox',\n",
       " ' born',\n",
       " ' on',\n",
       " ' 1',\n",
       " '/23',\n",
       " '/2013',\n",
       " ' jumped',\n",
       " ' over',\n",
       " ' the',\n",
       " ' lazy',\n",
       " ' dog',\n",
       " ' born',\n",
       " ' on',\n",
       " ' 10',\n",
       " '/6',\n",
       " '/10',\n",
       " '.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "split(r'\\M',t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'The ',\n",
       " 'quick ',\n",
       " 'brown ',\n",
       " 'fox ',\n",
       " 'born ',\n",
       " 'on ',\n",
       " '1/',\n",
       " '23/',\n",
       " '2013 ',\n",
       " 'jumped ',\n",
       " 'over ',\n",
       " 'the ',\n",
       " 'lazy ',\n",
       " 'dog ',\n",
       " 'born ',\n",
       " 'on ',\n",
       " '10/',\n",
       " '6/',\n",
       " '10.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "split(r'\\m',t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look-arounds\n",
    "We can split on any 4-letter word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox ',\n",
       " ' on 1/23/',\n",
       " ' jumped ',\n",
       " ' the ',\n",
       " ' dog ',\n",
       " ' on 10/6/10.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "split(r'(?x) \\b \\w{4} \\b', t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we want to split on any 4-letter word but **born**? We can use a look-ahead assertion.  Look-aheads and look-behinds come in two flavors: positive and negative.  All four are zero-width assertions.  They required certain characters to be present or absent, but don't consume the characters.  In this case, we need a negative assertion.  We could do a look-ahead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox born on 1/23/',\n",
       " ' jumped ',\n",
       " ' the ',\n",
       " ' dog born on 10/6/10.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(r'(?x) \\b (?!born) \\w{4} \\b', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or a look-behind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox born on 1/23/',\n",
       " ' jumped ',\n",
       " ' the ',\n",
       " ' dog born on 10/6/10.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(r'(?x) \\b \\w{4} (?<!born) \\b', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or, if we are feeling perverse, both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox born on 1/23/',\n",
       " ' jumped ',\n",
       " ' the ',\n",
       " ' dog born on 10/6/10.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(r'(?x) \\b (?!born) \\w{4} (?<!born) \\b', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one splits on any 4-letter word that doesn't contain **o**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox born on 1/23/',\n",
       " ' jumped over the ',\n",
       " ' dog born on 10/6/10.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "split(r'(?x) \\b (?!\\w*o) \\w{4} \\b', t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one splits on the letter **o**.  The **o** is consumed and lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick br',\n",
       " 'wn f',\n",
       " 'x b',\n",
       " 'rn ',\n",
       " 'n 1/23/2013 jumped ',\n",
       " 'ver the lazy d',\n",
       " 'g b',\n",
       " 'rn ',\n",
       " 'n 10/6/10.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(r'(?x) o', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one has a positive look-ahead assertion.  It splits before every **o**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick br',\n",
       " 'own f',\n",
       " 'ox b',\n",
       " 'orn ',\n",
       " 'on 1/23/2013 jumped ',\n",
       " 'over the lazy d',\n",
       " 'og b',\n",
       " 'orn ',\n",
       " 'on 10/6/10.']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(r'(?x) (?=o)', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one has a positive look-behind assertion.  It splits after evey **o**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick bro',\n",
       " 'wn fo',\n",
       " 'x bo',\n",
       " 'rn o',\n",
       " 'n 1/23/2013 jumped o',\n",
       " 'ver the lazy do',\n",
       " 'g bo',\n",
       " 'rn o',\n",
       " 'n 10/6/10.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "split(r'(?x) (?<=o)', t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one splits between  **o** and **r**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox bo',\n",
       " 'rn on 1/23/2013 jumped over the lazy dog bo',\n",
       " 'rn on 10/6/10.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(r'(?x) (?<=o) (?=r)', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assertions could appear in either order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox bo',\n",
       " 'rn on 1/23/2013 jumped over the lazy dog bo',\n",
       " 'rn on 10/6/10.']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(r'(?x) (?=r) (?<=o)', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one splits between any two consecutive vowels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The qu',\n",
       " 'ick brown fox born on 1/23/2013 jumped over the lazy dog born on 10/6/10.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(r'(?x) (?<=[aeiou]) (?=[aeiou])', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun with DNA: Open reading frames\n",
    "DNA is a sequence of bases, A, C, G, or T.  They are translated into proteins 3 bases at a time.  Each 3-base sequence is called a **codon**.  There is a special **start codon** ATG, and three **stop codons**, TGA, TAG, and TAA.  The start and stop codons are highlighted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dna = 'cgcgcATGcgcgcgTGAcgcgcgTAGcgcgcgcgc'\n",
    "dna = dna.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **opening reading frame** or **ORF** consists of a start codon, followed by some more codons, and ending with a stop codon.  (In real life, \"some more\" is usually hundreds or thousands.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<regex.Match object; span=(5, 26), match='atgcgcgcgtgacgcgcgtag'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orfpat = r'(?x) atg (...)* (tga|tag|taa)'\n",
    "search(orfpat,dna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, that's not quite right.  The internal codons should not be stop codons. We can handle that with a negative lookahead assertion.  (Can you think of another way?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<regex.Match object; span=(5, 17), match='atgcgcgcgtga'>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orfpat = r'(?x) atg  ( (?!tga|tag|taa) ... )*  (tga|tag|taa)'\n",
    "search(orfpat,dna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't really want to capture the \"some more codons\" separately.  In a minute that will get in the way.  So we can use **(?:...)** to group without capturing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atgcgcgcgtga']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orfpat = r'(?x) ( atg  (?: (?!tga|tag|taa) ... )*  (?:tga|tag|taa) )'\n",
    "findall(orfpat,dna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another DNA sequence.  Note that this one has overlapping ORFs.  We would like a list of **all** orfs, specifically **ATGcATGcgTGA** and **ATGcgTGAcTAA**.  Our last pattern only finds the first ORF. Since it consumes the first ORF, it also consumes the beginning of the second ORF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atgcatgcgtga']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dna = 'cgcgcATGcATGcgTGAcTAAcgTAGcgcgcgcgc'\n",
    "\n",
    "dna = dna.lower()\n",
    "\n",
    "findall(orfpat,dna)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to find something without consuming it, we can use a positive lookahead assertion.  We put the whole ORF pattern inside the lookahead.  We need to capture what is matched by the lookahead without consuming it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atgcatgcgtga', 'atgcgtgactaa']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "orfpat = r'(?x) (?= ( atg  (?: (?!tga|tag|taa) ... )*  (?:tga|tag|taa) ))'\n",
    "\n",
    "findall(orfpat,dna)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the position of the capturing parentheses.  This doesn't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "orfpat = r'(?x) ( (?= atg  (?: (?!tga|tag|taa) ... )*  (?:tga|tag|taa) ))'\n",
    "\n",
    "findall(orfpat,dna)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why? Because the look-ahead assertion has width 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More fun with DNA: Restriction Digest Assays\n",
    "To perform certain assays, molecular biologists subject DNA sequences to enzymes known as restriction enzymes. There are several types; this is about Type II restriction endonucleases, to be precise. They are usually named with three letters, for the species of origin, and a Roman numeral; e.g., AfeI comes from Alcaligenes faecalis.  These enzymes typically recognize a specific sequence of 6-10 letters, and cut the DNA somewhere in the middle of that sequence.  For example, BgIII recognizes **AGATCT** and cuts between the first **A** and the **G**.\n",
    "\n",
    "For a typical assay, the DNA will be digested by a \"cocktail\" of 3-6 enzymes. The lengths of the resulting pieces will be measured by gel electrophoresis.  The lengths should match up with the lengths predicted by an in silico digestion. If not, something is wrong.\n",
    "Our task is to do the in silico digestion.\n",
    "\n",
    "For development, here is a dictionary of 4 enzymes, a DNA sequence to digest, and the string we would like to get out of the process. In real life, the DNA sequence would be thousands to ten-thousands of letters long. The researcher could be interested in knowing the cut-points for dozens of enzymes, even though a typical assay uses just a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "enzymes = {'A-GATCT': 'BgIII',\n",
    "           'AGC-GCT': 'AfeI',\n",
    "           'AGG-CCT': 'StuI',\n",
    "           'AT-CGAT': 'ClaI'}\n",
    "\n",
    "dna = 'AAAAGCGCTAAAATCGATAAAAAAGATCTAAAAAGCGCT'\n",
    "\n",
    "goal = 'AAAAGC <AfeI> GCTAAAAT <ClaI> CGATAAAAAA <BgIII> GATCTAAAAAGC <AfeI> GCT'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use positive look-aheads and look-behinds.  We will build a look ahead-look behind combination for each enzyme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(?<=AT)(?=CGAT)', '(?<=A)(?=GATCT)', '(?<=AGC)(?=GCT)', '(?<=AGG)(?=CCT)']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pats = ['(?<=' + fore + ')(?=' + aft + ')' \n",
    "        \n",
    "        for (fore,aft) in [split(r'-',s) for s in enzymes.keys()]]\n",
    "\n",
    "pats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(?<=AT)(?=CGAT) | (?<=A)(?=GATCT) | (?<=AGC)(?=GCT) | (?<=AGG)(?=CCT)'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pats = ' | '.join(pats)\n",
    "pats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(?x) ( (?<=AT)(?=CGAT) | (?<=A)(?=GATCT) | (?<=AGC)(?=GCT) | (?<=AGG)(?=CCT) )'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pat = \"(?x) ( \" + pats + \" )\"\n",
    "pat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAAAGC <AfeI> GCTAAAAT <ClaI> CGATAAAAAA <BgIII> GATCTAAAAAGC <AfeI> GCT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AAAAGC', '', 'GCTAAAAT', '', 'CGATAAAAAA', '', 'GATCTAAAAAGC', '', 'GCT']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(goal)\n",
    "\n",
    "split(pat,dna)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good start.  We split in the right places, but we didn't capture the recognition sequences, so we can't retrieve the name of the enzyme from the dictionary. In fact, we captured empty strings.  That's because we captured a zero-width assertion. So we will add some parentheses to capture the look-aheads and look-behinds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(?x) (?: (?<=(AT)) (?=(CGAT))  |  (?<=(A)) (?=(GATCT))  |  (?<=(AGC)) (?=(GCT))  |  (?<=(AGG)) (?=(CCT)) )'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pats = ['(?<=(' + fore + ')) (?=(' + aft + '))' \n",
    "        \n",
    "        for (fore,aft) in [split(r'-',s) for s in enzymes.keys()]]\n",
    "\n",
    "pats = '  |  '.join(pats)\n",
    "pat = '(?x) (?: ' + pats + ' )'\n",
    "pat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAAAGC',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'AGC',\n",
       " 'GCT',\n",
       " None,\n",
       " None,\n",
       " 'GCTAAAAT',\n",
       " 'AT',\n",
       " 'CGAT',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'CGATAAAAAA',\n",
       " None,\n",
       " None,\n",
       " 'A',\n",
       " 'GATCT',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'GATCTAAAAAGC',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'AGC',\n",
       " 'GCT',\n",
       " None,\n",
       " None,\n",
       " 'GCT']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "split(pat,dna)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?  The pattern has eight sets of capturing parentheses. So, the match also returns eight groups when it's executed.  Only the parentheses from the successful alternative will capture anything.  The other six groups are set to **None**.\n",
    "\n",
    "Happily, **regex** provides a new \"branch reset\" feature. Briefly, it means that capturing occurs only on the successful branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(?x) (?| (?<=(AT)) (?=(CGAT))  |  (?<=(A)) (?=(GATCT))  |  (?<=(AGC)) (?=(GCT))  |  (?<=(AGG)) (?=(CCT)) )'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pat = '(?x) (?| ' + pats + ' )'\n",
    "pat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAAAGC',\n",
       " 'AGC',\n",
       " 'GCT',\n",
       " 'GCTAAAAT',\n",
       " 'AT',\n",
       " 'CGAT',\n",
       " 'CGATAAAAAA',\n",
       " 'A',\n",
       " 'GATCT',\n",
       " 'GATCTAAAAAGC',\n",
       " 'AGC',\n",
       " 'GCT',\n",
       " 'GCT']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "split(pat,dna)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray!  Now all we have to do it to map the recognition sequences into enzyme names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AAAAGC <AfeI> GCTAAAAT <ClaI> CGATAAAAAA <BgIII> GATCTAAAAAGC <AfeI> GCT'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "L = split(pat,dna)\n",
    "\n",
    "LL = [ ' <' + enzymes[L[i]+'-'+L[i+1]] + '> '+ L[i+2] \n",
    "      \n",
    "                  for i in range(1,len(L),3) ]\n",
    "\n",
    "L[0] + ''.join(LL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can pull it all together into a nice class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import regex as re\n",
    "\n",
    "class EndonucleaseDigestor:\n",
    "    \n",
    "    def __init__(this,enzymeDict):\n",
    "        pats = ['(?<=(' + fore + '))(?=(' + aft + '))' \n",
    "                for (fore,aft) in [re.split(r'-',s) for s in enzymeDict.keys()]]\n",
    "        pat = ' | '.join(pats)\n",
    "        pat = '(?x) (?| ' + pat + ' )'\n",
    "        this.pat = re.compile(pat)\n",
    "        this.enzymes = enzymeDict\n",
    "        \n",
    "    def digest(this,dna):\n",
    "        L = this.pat.split(dna)\n",
    "        LL = [ ' <' + enzymes[L[i]+'-'+L[i+1]] + '> '+ L[i+2] for i in range(1,len(L),3) ]\n",
    "        return L[0] + ''.join(LL)\n",
    "        \n",
    "\n",
    "enzymes = {'A-GATCT': 'BgIII',\n",
    "           'AGC-GCT': 'AfeI',\n",
    "           'AGG-CCT': 'StuI',\n",
    "           'AT-CGAT': 'ClaI'}\n",
    "dna = 'AAAAGCGCTAAAATCGATAAAAAAGATCTAAAAAGCGCT'\n",
    "goal = 'AAAAGC <AfeI> GCTAAAAT <ClaI> CGATAAAAAA <BgIII> GATCTAAAAAGC <AfeI> GCT'\n",
    "\n",
    "digestor = EndonucleaseDigestor(enzymes)\n",
    "if digestor.digest(dna) == goal: print(\"passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TMTOWTDT: **sub** with a function\n",
    "There's another way to solve the restriction digest problem. This time let's start by building a dictionary that maps the recognition sequences into versions with the enzyme name interposed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AGATCT': 'A <BgIII> GATCT',\n",
       " 'AGCGCT': 'AGC <AfeI> GCT',\n",
       " 'AGGCCT': 'AGG <StuI> CCT',\n",
       " 'ATCGAT': 'AT <ClaI> CGAT'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "d = { sub('-','',k) : sub('-',\" <\"+v+\"> \", k) for k,v in enzymes.items()}\n",
    "d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's build a pattern that matches all the recognition sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(?x) (ATCGAT | AGATCT | AGCGCT | AGGCCT)'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "p = ' | '.join( [ sub('-','',k) for k in enzymes.keys()])\n",
    "p = '(?x) (' + p + ')'\n",
    "p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second argument to **sub** can be a function rather than a string.  If so, the function is called with a **match** object as its argument.  We are interested in the first (and only) thing captured in the match and we want to get the corresponding string out of dictionary **d**.  So we define a function to do that.  Then we call sub with that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<regex.Match object; span=(3, 9), match='AGCGCT'>\n",
      "<regex.Match object; span=(12, 18), match='ATCGAT'>\n",
      "<regex.Match object; span=(23, 29), match='AGATCT'>\n",
      "<regex.Match object; span=(33, 39), match='AGCGCT'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AAAAGC <AfeI> GCTAAAAT <ClaI> CGATAAAAAA <BgIII> GATCTAAAAAGC <AfeI> GCT'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def subber (m):\n",
    "    print(m)\n",
    "    return d[m.group(1)]\n",
    "\n",
    "sub(p, subber, dna)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can make a class like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import regex as re\n",
    "\n",
    "class AnotherEndonucleaseDigestor:\n",
    "    \n",
    "    def __init__(this,enzymeDict):\n",
    "        this.d = { re.sub('-','',k) : re.sub('-',\" <\"+v+\"> \", k) for k,v in enzymes.items()}\n",
    "        p = ' | '.join( [ re.sub('-','',k) for k in enzymes.keys()])\n",
    "        p = '(?x) (' + p + ')'\n",
    "        this.pat = re.compile(p)\n",
    "        this.enzymes = enzymeDict\n",
    "        \n",
    "    def digest(this,dna):\n",
    "        return this.pat.sub( lambda m: this.d[m.group(1)]  , dna)\n",
    "\n",
    "enzymes = {'A-GATCT': 'BgIII',\n",
    "           'AGC-GCT': 'AfeI',\n",
    "           'AGG-CCT': 'StuI',\n",
    "           'AT-CGAT': 'ClaI'}\n",
    "dna = 'AAAAGCGCTAAAATCGATAAAAAAGATCTAAAAAGCGCT'\n",
    "goal = 'AAAAGC <AfeI> GCTAAAAT <ClaI> CGATAAAAAA <BgIII> GATCTAAAAAGC <AfeI> GCT'\n",
    "\n",
    "digestor = AnotherEndonucleaseDigestor(enzymes)\n",
    "if digestor.digest(dna) == goal: print(\"passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That probably seems a lot simpler, but there is one problem.  What if two recognition sites are overlapping?  For the in silico simulation of a real digest, it doesn't much matter, because the resolution of gel electrophoresis is much less than the 5-10 bases that might be overlapping.  On the other hand, if the scientist actually wants a complete inventory of all the restriction sites for a large set of enzymes, overlaps matter, and this solution won't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested sets\n",
    "We have seen some character sets such as **[aeiou]** for all (lower-case) vowels.  Suppose we want all lower-case consonants.  One obvious way is to list them all.  We might also be tempted to use set negation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Th',\n",
       " ' q',\n",
       " 'ck br',\n",
       " 'wn f',\n",
       " 'x b',\n",
       " 'rn ',\n",
       " 'n 1/23/2013 j',\n",
       " 'mp',\n",
       " 'd ',\n",
       " 'v',\n",
       " 'r th',\n",
       " ' l',\n",
       " 'zy d',\n",
       " 'g b',\n",
       " 'rn ',\n",
       " 'n 10/6/10.']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findall(r'[^aeiou]+',t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that we get not just consonants, but spaces, digits, etc.\n",
    "The new **regex** module allows us to do arithmetic on sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h',\n",
       " 'q',\n",
       " 'ck',\n",
       " 'br',\n",
       " 'wn',\n",
       " 'f',\n",
       " 'x',\n",
       " 'b',\n",
       " 'rn',\n",
       " 'n',\n",
       " 'j',\n",
       " 'mp',\n",
       " 'd',\n",
       " 'v',\n",
       " 'r',\n",
       " 'th',\n",
       " 'l',\n",
       " 'zy',\n",
       " 'd',\n",
       " 'g',\n",
       " 'b',\n",
       " 'rn',\n",
       " 'n']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findall(r'(?x) [[a-z]--[aeiou]]+', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy matching\n",
    "With **regex**, you can specify that patterns need only be satisfied approximately.  You can specify the number of insertions (**i**), number of deletions (**d**), and number of substitutions (**s**) as well as total number of errors (**e**).\n",
    "This example allows at most one insertion and at most one deletion for each pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<regex.Match object; span=(10, 19), match='crown fax', fuzzy_counts=(0, 2, 2)>,\n",
       " <regex.Match object; span=(52, 61), match='leazy hog', fuzzy_counts=(0, 2, 1)>]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(finditer(r'(brown|lazy){i<=1,d<=1} (dog|fox){i<=1,d<=1}',\n",
    "'The quick crown fax barn on Monday jumped over the sleazy hog bran on Tuesday.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the match object reports the number of insertions, deletions, and substitutions as **fuzzy_counts**. \n",
    "\n",
    "You can even **require** a minimum number of errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<regex.Match object; span=(8, 19), match='k crown fax', fuzzy_counts=(2, 2, 0)>,\n",
       " <regex.Match object; span=(20, 27), match='barn on', fuzzy_counts=(3, 0, 2)>,\n",
       " <regex.Match object; span=(50, 61), match=' sleazy hog', fuzzy_counts=(1, 3, 0)>,\n",
       " <regex.Match object; span=(62, 69), match='bran on', fuzzy_counts=(3, 0, 2)>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(finditer(r'(brown|lazy){1<=e<=3} (dog|fox){1<=e<=2}',\n",
    "'The quick crown fax barn on Monday jumped over the sleazy hog bran on Tuesday.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What matched what?  We can find out by doing some more capturing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k crown', '', '', 'fax'),\n",
       " ('barn', '', 'on', ''),\n",
       " ('', ' sleazy', 'hog', ''),\n",
       " ('bran', '', 'on', '')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findall(r'(?:(brown)|(lazy)){1<=e<=3} (?:(dog)|(fox)){1<=e<=2}',\n",
    "'The quick crown fax barn on Monday jumped over the sleazy hog bran on Tuesday.')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we try our orginal correct string? We should get back no matches, because there are no errors, right?  Maybe not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ck brown', '', 'fox', ''),\n",
       " (' born', '', 'on', ''),\n",
       " ('', 'he lazy', 'dog', ''),\n",
       " ('born', '', 'on', '')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "findall(r'(?:(brown)|(lazy)){1<=e<=3} (?:(dog)|(fox)){1<=e<=2}',\n",
    "        \n",
    "        'The quick brown fox born on 1/23/2013 jumped over the lazy dog born on 10/6/10.')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe it will work if we try the **BESTMATCH** option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' brown', 'fox '), (' lazy', 'dog '), ('born', 'on')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "findall(r'(brown|lazy){1<=e<=3} (dog|fox){1<=e<=2}',\n",
    "        'The quick brown fox born on 1/23/2013 jumped over the lazy dog born on 10/6/10.',\n",
    "        BESTMATCH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.  Maybe we need the **ENHANCEMATCH** option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' brown', 'fox'), ('born', 'on'), (' lazy', 'dog '), ('born', 'on')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "findall(r'(brown|lazy){1<=e<=3} (dog|fox){1<=e<=2}',\n",
    "        'The quick brown fox born on 1/23/2013 jumped over the lazy dog born on 10/6/10.',\n",
    "         ENHANCEMATCH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we should use both...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' brown', 'fox '), (' lazy', 'dog '), ('born', 'on')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "findall(r'(brown|lazy){1<=e<=3} (dog|fox){1<=e<=2}',\n",
    "        'The quick brown fox born on 1/23/2013 jumped over the lazy dog born on 10/6/10.',\n",
    "         ENHANCEMATCH | BESTMATCH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's try a spelling corrector.  Here's a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aarhus\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('words.txt')\n",
    "\n",
    "f.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aaron\\n',\n",
       " 'Ababa\\n',\n",
       " 'aback\\n',\n",
       " 'abaft\\n',\n",
       " 'abandon\\n',\n",
       " 'abandoned\\n',\n",
       " 'abandoning\\n',\n",
       " 'abandonment\\n',\n",
       " 'abandons\\n',\n",
       " 'abase\\n']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = f.readlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zoom zooms zoos Zorn Zoroaster Zoroastrian Zulu Zulus Zurich'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words =  ' '.join( [sub('\\n', '', w) for w in words] )\n",
    "words[-60:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a string with some misspelled (and correct) words. It might seem counterintuitive, but we will take the misspelled words and turn them into a pattern, and use the dictionary as the target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(?x) \\\\m (?: (abrogatting){e<=2} | (baandoned){e<=2} | (abreviat){e<=2} | (astracted){e<=2} | (absinthe){e<=2} | (abussed){e<=2} | (abus){e<=2} | (zoan){e<=2} ) \\\\M'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "misspelt = 'abrogatting baandoned abreviat astracted absinthe abussed abus zoan'\n",
    "\n",
    "misspelt = split('\\W+', misspelt)\n",
    "\n",
    "misspelt = [r\"(\" + s + r\"){e<=2}\" for s in misspelt]\n",
    "\n",
    "misspelt = r\"(?x) \\m (?: \" + \" | \".join(misspelt) + r\" ) \\M\"\n",
    "\n",
    "misspelt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this time we did not use the brach reset feature.  That's because the captured empty strings are going to tell us which misspelled word was matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'abandoned', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', 'abase', ''),\n",
       " ('', '', '', '', '', 'abased', '', ''),\n",
       " ('', '', '', '', '', '', 'abash', ''),\n",
       " ('', '', '', '', '', 'abashed', '', ''),\n",
       " ('', '', '', '', '', '', 'abbe', ''),\n",
       " ('', '', 'abbreviate', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', 'abed', ''),\n",
       " ('', '', '', '', '', '', 'abet', ''),\n",
       " ('', '', '', '', '', '', 'abets', ''),\n",
       " ('', '', '', '', '', '', 'able', ''),\n",
       " ('', '', '', '', '', '', 'ably', ''),\n",
       " ('', '', '', '', '', '', 'Abos', ''),\n",
       " ('', '', '', '', '', '', 'about', ''),\n",
       " ('abrogating', '', '', '', '', '', '', ''),\n",
       " ('', '', '', '', 'absentee', '', '', ''),\n",
       " ('', '', '', '', 'absinthe', '', '', ''),\n",
       " ('', '', '', 'abstracted', '', '', '', ''),\n",
       " ('', '', '', '', '', '', 'Abu', ''),\n",
       " ('', '', '', '', '', '', 'abuse', ''),\n",
       " ('', '', '', '', '', 'abused', '', ''),\n",
       " ('', '', '', '', '', 'abuses', '', ''),\n",
       " ('', '', '', '', '', '', 'abut', ''),\n",
       " ('', '', '', '', '', '', 'abuts', ''),\n",
       " ('', '', '', '', '', 'abutted', '', ''),\n",
       " ('', '', '', '', '', '', 'abyss', ''),\n",
       " ('', '', '', '', '', 'abysses', '', ''),\n",
       " ('', '', '', '', '', '', 'aces', ''),\n",
       " ('', '', '', '', '', '', 'adds', ''),\n",
       " ('', '', '', '', '', '', 'ads', ''),\n",
       " ('', '', '', '', '', '', 'ages', ''),\n",
       " ('', '', '', '', '', '', 'ague', ''),\n",
       " ('', '', '', '', '', '', 'aids', ''),\n",
       " ('', '', '', '', '', '', 'aims', ''),\n",
       " ('', '', '', '', '', '', 'airs', ''),\n",
       " ('', '', '', '', '', '', '', 'Alan'),\n",
       " ('', '', '', '', '', '', 'alas', ''),\n",
       " ('', '', '', '', '', '', 'album', ''),\n",
       " ('', '', '', '', '', '', 'albums', ''),\n",
       " ('', '', '', '', '', '', 'alms', ''),\n",
       " ('', '', '', '', '', '', 'alum', ''),\n",
       " ('', '', '', '', '', 'amassed', '', ''),\n",
       " ('', '', '', '', '', '', 'ambush', ''),\n",
       " ('', '', '', '', '', 'ambushed', '', ''),\n",
       " ('', '', '', '', '', '', 'amuse', ''),\n",
       " ('', '', '', '', '', 'amused', '', ''),\n",
       " ('', '', '', '', '', '', '', 'an'),\n",
       " ('', '', '', '', '', '', 'ants', ''),\n",
       " ('', '', '', '', '', '', 'anus', ''),\n",
       " ('', '', '', '', '', '', 'apes', ''),\n",
       " ('', '', '', '', '', '', 'aqua', ''),\n",
       " ('', '', '', '', '', '', 'arcs', ''),\n",
       " ('', '', '', '', '', '', 'arms', ''),\n",
       " ('arrogating', '', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', 'arts', ''),\n",
       " ('', '', '', '', '', '', 'as', ''),\n",
       " ('', '', '', '', '', '', 'asks', ''),\n",
       " ('', '', '', '', '', '', 'ass', ''),\n",
       " ('', '', '', 'attracted', '', '', '', ''),\n",
       " ('', '', '', '', '', '', 'awls', ''),\n",
       " ('', '', '', '', '', '', 'axes', ''),\n",
       " ('', '', '', '', '', '', 'axis', ''),\n",
       " ('', '', '', '', '', '', 'ayes', ''),\n",
       " ('', '', '', '', '', '', 'babes', ''),\n",
       " ('', '', '', '', '', '', 'Babul', ''),\n",
       " ('', '', '', '', '', '', '', 'ban'),\n",
       " ('', '', '', '', '', '', '', 'bean'),\n",
       " ('', '', '', '', '', '', '', 'boa'),\n",
       " ('', '', '', '', '', '', '', 'boar'),\n",
       " ('', '', '', '', '', '', '', 'boat'),\n",
       " ('', '', '', '', '', '', '', 'Bonn'),\n",
       " ('', '', '', '', '', '', '', 'boon'),\n",
       " ('', '', '', '', '', '', '', 'born'),\n",
       " ('', '', '', '', '', 'bossed', '', ''),\n",
       " ('', '', '', '', '', '', '', 'bran'),\n",
       " ('', '', '', '', '', '', 'bud', ''),\n",
       " ('', '', '', '', '', '', 'buds', ''),\n",
       " ('', '', '', '', '', '', 'bug', ''),\n",
       " ('', '', '', '', '', '', 'bugs', ''),\n",
       " ('', '', '', '', '', '', 'bum', ''),\n",
       " ('', '', '', '', '', '', 'bums', ''),\n",
       " ('', '', '', '', '', '', 'bun', ''),\n",
       " ('', '', '', '', '', '', 'buns', ''),\n",
       " ('', '', '', '', '', '', 'bus', ''),\n",
       " ('', '', '', '', '', 'bused', '', ''),\n",
       " ('', '', '', '', '', '', 'bush', ''),\n",
       " ('', '', '', '', '', 'busied', '', ''),\n",
       " ('', '', '', '', '', '', 'buss', ''),\n",
       " ('', '', '', '', '', 'bussed', '', ''),\n",
       " ('', '', '', '', '', 'busses', '', ''),\n",
       " ('', '', '', '', '', '', 'bust', ''),\n",
       " ('', '', '', '', '', 'busted', '', ''),\n",
       " ('', '', '', '', '', '', 'busy', ''),\n",
       " ('', '', '', '', '', '', 'but', ''),\n",
       " ('', '', '', '', '', '', 'buy', ''),\n",
       " ('', '', '', '', '', '', 'buys', ''),\n",
       " ('', '', '', '', '', '', 'cabs', ''),\n",
       " ('', '', '', '', '', '', '', 'can'),\n",
       " ('', '', '', '', '', '', '', 'clan'),\n",
       " ('', '', '', '', '', '', '', 'coal'),\n",
       " ('', '', '', '', '', '', '', 'coat'),\n",
       " ('', '', '', '', '', '', '', 'coax'),\n",
       " ('', '', '', '', '', '', '', 'Cohn'),\n",
       " ('', '', '', '', '', '', '', 'coin'),\n",
       " ('', '', '', '', '', '', '', 'con'),\n",
       " ('', '', '', '', '', '', '', 'coon'),\n",
       " ('', '', '', '', '', '', '', 'corn'),\n",
       " ('', '', '', '', '', '', '', 'Cowan'),\n",
       " ('', '', '', '', '', '', '', 'Dan'),\n",
       " ('', '', '', '', '', '', '', 'dean'),\n",
       " ('', '', '', '', '', '', 'deus', ''),\n",
       " ('', '', '', 'distracted', '', '', '', ''),\n",
       " ('', '', '', '', '', '', '', 'Dolan'),\n",
       " ('', '', '', '', '', '', '', 'don'),\n",
       " ('', '', '', '', '', '', '', 'down'),\n",
       " ('', '', '', '', '', '', 'ebbs', ''),\n",
       " ('', '', '', '', '', '', '', 'Egan'),\n",
       " ('', '', '', 'extracted', '', '', '', ''),\n",
       " ('', '', '', '', '', '', '', 'fan'),\n",
       " ('', '', '', '', '', '', '', 'foal'),\n",
       " ('', '', '', '', '', '', '', 'foam'),\n",
       " ('', '', '', '', '', '', '', 'Fran'),\n",
       " ('', '', '', '', '', '', '', 'Goa'),\n",
       " ('', '', '', '', '', '', '', 'goad'),\n",
       " ('', '', '', '', '', '', '', 'goal'),\n",
       " ('', '', '', '', '', '', '', 'goat'),\n",
       " ('', '', '', '', '', '', '', 'gown'),\n",
       " ('', '', '', '', '', '', '', 'groan'),\n",
       " ('', '', '', '', '', '', 'Gus', ''),\n",
       " ('', '', '', '', '', '', '', 'Han'),\n",
       " ('', '', '', '', '', '', '', 'hoar'),\n",
       " ('', '', '', '', '', '', '', 'Hokan'),\n",
       " ('', '', '', '', '', '', '', 'horn'),\n",
       " ('', '', '', '', '', '', '', 'Ian'),\n",
       " ('', '', '', '', '', '', 'ibis', ''),\n",
       " ('', '', '', '', '', '', '', 'ion'),\n",
       " ('', '', '', '', '', '', '', 'Iran'),\n",
       " ('', '', '', '', '', '', '', 'Ivan'),\n",
       " ('', '', '', '', '', '', 'jabs', ''),\n",
       " ('', '', '', '', '', '', 'Janus', ''),\n",
       " ('', '', '', '', '', '', '', 'jean'),\n",
       " ('', '', '', '', '', '', '', 'Joan'),\n",
       " ('', '', '', '', '', '', '', 'John'),\n",
       " ('', '', '', '', '', '', '', 'join'),\n",
       " ('', '', '', '', '', '', '', 'Jon'),\n",
       " ('', '', '', '', '', '', '', 'Juan'),\n",
       " ('', '', '', '', '', '', 'Kabul', ''),\n",
       " ('', '', '', '', '', '', '', 'Klan'),\n",
       " ('', '', '', '', '', '', '', 'Koran'),\n",
       " ('', '', '', '', '', '', 'labs', ''),\n",
       " ('', '', '', '', '', '', '', 'lean'),\n",
       " ('', '', '', '', '', '', '', 'load'),\n",
       " ('', '', '', '', '', '', '', 'loaf'),\n",
       " ('', '', '', '', '', '', '', 'loan'),\n",
       " ('', '', '', '', '', '', '', 'loans'),\n",
       " ('', '', '', '', '', '', '', 'Logan'),\n",
       " ('', '', '', '', '', '', '', 'loin'),\n",
       " ('', '', '', '', '', '', '', 'loon'),\n",
       " ('', '', '', '', '', '', '', 'man'),\n",
       " ('', '', '', '', '', '', '', 'mean'),\n",
       " ('', '', '', '', '', '', '', 'moan'),\n",
       " ('', '', '', '', '', '', '', 'moans'),\n",
       " ('', '', '', '', '', '', '', 'moat'),\n",
       " ('', '', '', '', '', '', '', 'Moen'),\n",
       " ('', '', '', '', '', '', '', 'Moon'),\n",
       " ('', '', '', '', '', '', '', 'Moran'),\n",
       " ('', '', '', '', '', '', '', 'morn'),\n",
       " ('', '', '', '', '', '', '', 'Nan'),\n",
       " ('', '', '', '', '', '', '', 'Noah'),\n",
       " ('', '', '', '', '', '', '', 'Nolan'),\n",
       " ('', '', '', '', '', '', '', 'non'),\n",
       " ('', '', '', '', '', '', '', 'noon'),\n",
       " ('', '', '', '', '', '', '', 'noun'),\n",
       " ('', '', '', '', '', '', '', 'oaf'),\n",
       " ('', '', '', '', '', '', '', 'oak'),\n",
       " ('', '', '', '', '', '', '', 'oar'),\n",
       " ('', '', '', '', '', '', '', 'oat'),\n",
       " ('', '', '', '', '', '', '', 'Oman'),\n",
       " ('', '', '', '', '', '', '', 'on'),\n",
       " ('', '', '', '', '', '', 'onus', ''),\n",
       " ('', '', '', '', '', '', 'opus', ''),\n",
       " ('', '', '', '', '', '', '', 'own'),\n",
       " ('', '', '', '', '', '', '', 'pan'),\n",
       " ('', '', '', '', '', '', 'Pius', ''),\n",
       " ('', '', '', '', '', '', '', 'plan'),\n",
       " ('', '', '', '', '', '', 'plus', ''),\n",
       " ('', '', '', '', '', '', 'pus', ''),\n",
       " ('', '', '', '', '', '', '', 'ran'),\n",
       " ('', '', '', 'retracted', '', '', '', ''),\n",
       " ('', '', '', '', '', '', '', 'road'),\n",
       " ('', '', '', '', '', '', '', 'roam'),\n",
       " ('', '', '', '', '', '', '', 'roar'),\n",
       " ('', '', '', '', '', '', '', 'Roman'),\n",
       " ('', '', '', '', '', '', '', 'Ron'),\n",
       " ('', '', '', '', '', '', '', 'Ryan'),\n",
       " ('', '', '', '', '', '', '', 'San'),\n",
       " ('', '', '', '', '', '', '', 'scan'),\n",
       " ('', '', '', '', '', '', '', 'Sean'),\n",
       " ('', '', '', '', '', '', '', 'Sian'),\n",
       " ('', '', '', '', '', '', '', 'Sloan'),\n",
       " ('', '', '', '', '', '', '', 'soak'),\n",
       " ('', '', '', '', '', '', '', 'soap'),\n",
       " ('', '', '', '', '', '', '', 'soar'),\n",
       " ('', '', '', '', '', '', '', 'son'),\n",
       " ('', '', '', '', '', '', '', 'soon'),\n",
       " ('', '', '', '', '', '', '', 'sown'),\n",
       " ('', '', '', '', '', '', '', 'span'),\n",
       " ('', '', '', '', '', '', '', 'Stan'),\n",
       " ('', '', '', '', '', '', 'Sus', ''),\n",
       " ('', '', '', '', '', '', '', 'swan'),\n",
       " ('', '', '', '', '', '', 'tabs', ''),\n",
       " ('', '', '', '', '', '', '', 'tan'),\n",
       " ('', '', '', '', '', '', '', 'than'),\n",
       " ('', '', '', '', '', '', 'thus', ''),\n",
       " ('', '', '', '', '', '', '', 'toad'),\n",
       " ('', '', '', '', '', '', '', 'ton'),\n",
       " ('', '', '', '', '', '', '', 'torn'),\n",
       " ('', '', '', '', '', '', '', 'town'),\n",
       " ('', '', '', '', '', '', '', 'Ulan'),\n",
       " ('', '', '', '', '', '', 'us', ''),\n",
       " ('', '', '', '', '', '', '', 'van'),\n",
       " ('', '', '', '', '', '', '', 'wan'),\n",
       " ('', '', '', '', '', '', '', 'wean'),\n",
       " ('', '', '', '', '', '', '', 'woman'),\n",
       " ('', '', '', '', '', '', '', 'won'),\n",
       " ('', '', '', '', '', '', '', 'worn'),\n",
       " ('', '', '', '', '', '', '', 'Wotan'),\n",
       " ('', '', '', '', '', '', '', 'yon'),\n",
       " ('', '', '', '', '', '', '', 'Zan'),\n",
       " ('', '', '', '', '', '', '', 'zeal'),\n",
       " ('', '', '', '', '', '', 'Zeus', ''),\n",
       " ('', '', '', '', '', '', '', 'zonal'),\n",
       " ('', '', '', '', '', '', '', 'zone'),\n",
       " ('', '', '', '', '', '', '', 'zoo'),\n",
       " ('', '', '', '', '', '', '', 'zoom'),\n",
       " ('', '', '', '', '', '', '', 'zoos'),\n",
       " ('', '', '', '', '', '', '', 'Zorn')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lis = findall(misspelt,words, ENHANCEMATCH)\n",
    "\n",
    "lis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will transpose the matrix.  Every column will contain matches for a single misspelled word.  Most of the entries will be empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'abrogating',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'arrogating',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''),\n",
       " ('abandoned',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''),\n",
       " ('',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'abbreviate',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''),\n",
       " ('',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'abstracted',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'attracted',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'distracted',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'extracted',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'retracted',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''),\n",
       " ('',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'absentee',\n",
       "  'absinthe',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''),\n",
       " ('',\n",
       "  '',\n",
       "  'abased',\n",
       "  '',\n",
       "  'abashed',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'abused',\n",
       "  'abuses',\n",
       "  '',\n",
       "  '',\n",
       "  'abutted',\n",
       "  '',\n",
       "  'abysses',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'amassed',\n",
       "  '',\n",
       "  'ambushed',\n",
       "  '',\n",
       "  'amused',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'bossed',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'bused',\n",
       "  '',\n",
       "  'busied',\n",
       "  '',\n",
       "  'bussed',\n",
       "  'busses',\n",
       "  '',\n",
       "  'busted',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''),\n",
       " ('',\n",
       "  'abase',\n",
       "  '',\n",
       "  'abash',\n",
       "  '',\n",
       "  'abbe',\n",
       "  '',\n",
       "  'abed',\n",
       "  'abet',\n",
       "  'abets',\n",
       "  'able',\n",
       "  'ably',\n",
       "  'Abos',\n",
       "  'about',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Abu',\n",
       "  'abuse',\n",
       "  '',\n",
       "  '',\n",
       "  'abut',\n",
       "  'abuts',\n",
       "  '',\n",
       "  'abyss',\n",
       "  '',\n",
       "  'aces',\n",
       "  'adds',\n",
       "  'ads',\n",
       "  'ages',\n",
       "  'ague',\n",
       "  'aids',\n",
       "  'aims',\n",
       "  'airs',\n",
       "  '',\n",
       "  'alas',\n",
       "  'album',\n",
       "  'albums',\n",
       "  'alms',\n",
       "  'alum',\n",
       "  '',\n",
       "  'ambush',\n",
       "  '',\n",
       "  'amuse',\n",
       "  '',\n",
       "  '',\n",
       "  'ants',\n",
       "  'anus',\n",
       "  'apes',\n",
       "  'aqua',\n",
       "  'arcs',\n",
       "  'arms',\n",
       "  '',\n",
       "  'arts',\n",
       "  'as',\n",
       "  'asks',\n",
       "  'ass',\n",
       "  '',\n",
       "  'awls',\n",
       "  'axes',\n",
       "  'axis',\n",
       "  'ayes',\n",
       "  'babes',\n",
       "  'Babul',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'bud',\n",
       "  'buds',\n",
       "  'bug',\n",
       "  'bugs',\n",
       "  'bum',\n",
       "  'bums',\n",
       "  'bun',\n",
       "  'buns',\n",
       "  'bus',\n",
       "  '',\n",
       "  'bush',\n",
       "  '',\n",
       "  'buss',\n",
       "  '',\n",
       "  '',\n",
       "  'bust',\n",
       "  '',\n",
       "  'busy',\n",
       "  'but',\n",
       "  'buy',\n",
       "  'buys',\n",
       "  'cabs',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'deus',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'ebbs',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Gus',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'ibis',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'jabs',\n",
       "  'Janus',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Kabul',\n",
       "  '',\n",
       "  '',\n",
       "  'labs',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'onus',\n",
       "  'opus',\n",
       "  '',\n",
       "  '',\n",
       "  'Pius',\n",
       "  '',\n",
       "  'plus',\n",
       "  'pus',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Sus',\n",
       "  '',\n",
       "  'tabs',\n",
       "  '',\n",
       "  '',\n",
       "  'thus',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'us',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Zeus',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''),\n",
       " ('',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Alan',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'an',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'ban',\n",
       "  'bean',\n",
       "  'boa',\n",
       "  'boar',\n",
       "  'boat',\n",
       "  'Bonn',\n",
       "  'boon',\n",
       "  'born',\n",
       "  '',\n",
       "  'bran',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'can',\n",
       "  'clan',\n",
       "  'coal',\n",
       "  'coat',\n",
       "  'coax',\n",
       "  'Cohn',\n",
       "  'coin',\n",
       "  'con',\n",
       "  'coon',\n",
       "  'corn',\n",
       "  'Cowan',\n",
       "  'Dan',\n",
       "  'dean',\n",
       "  '',\n",
       "  '',\n",
       "  'Dolan',\n",
       "  'don',\n",
       "  'down',\n",
       "  '',\n",
       "  'Egan',\n",
       "  '',\n",
       "  'fan',\n",
       "  'foal',\n",
       "  'foam',\n",
       "  'Fran',\n",
       "  'Goa',\n",
       "  'goad',\n",
       "  'goal',\n",
       "  'goat',\n",
       "  'gown',\n",
       "  'groan',\n",
       "  '',\n",
       "  'Han',\n",
       "  'hoar',\n",
       "  'Hokan',\n",
       "  'horn',\n",
       "  'Ian',\n",
       "  '',\n",
       "  'ion',\n",
       "  'Iran',\n",
       "  'Ivan',\n",
       "  '',\n",
       "  '',\n",
       "  'jean',\n",
       "  'Joan',\n",
       "  'John',\n",
       "  'join',\n",
       "  'Jon',\n",
       "  'Juan',\n",
       "  '',\n",
       "  'Klan',\n",
       "  'Koran',\n",
       "  '',\n",
       "  'lean',\n",
       "  'load',\n",
       "  'loaf',\n",
       "  'loan',\n",
       "  'loans',\n",
       "  'Logan',\n",
       "  'loin',\n",
       "  'loon',\n",
       "  'man',\n",
       "  'mean',\n",
       "  'moan',\n",
       "  'moans',\n",
       "  'moat',\n",
       "  'Moen',\n",
       "  'Moon',\n",
       "  'Moran',\n",
       "  'morn',\n",
       "  'Nan',\n",
       "  'Noah',\n",
       "  'Nolan',\n",
       "  'non',\n",
       "  'noon',\n",
       "  'noun',\n",
       "  'oaf',\n",
       "  'oak',\n",
       "  'oar',\n",
       "  'oat',\n",
       "  'Oman',\n",
       "  'on',\n",
       "  '',\n",
       "  '',\n",
       "  'own',\n",
       "  'pan',\n",
       "  '',\n",
       "  'plan',\n",
       "  '',\n",
       "  '',\n",
       "  'ran',\n",
       "  '',\n",
       "  'road',\n",
       "  'roam',\n",
       "  'roar',\n",
       "  'Roman',\n",
       "  'Ron',\n",
       "  'Ryan',\n",
       "  'San',\n",
       "  'scan',\n",
       "  'Sean',\n",
       "  'Sian',\n",
       "  'Sloan',\n",
       "  'soak',\n",
       "  'soap',\n",
       "  'soar',\n",
       "  'son',\n",
       "  'soon',\n",
       "  'sown',\n",
       "  'span',\n",
       "  'Stan',\n",
       "  '',\n",
       "  'swan',\n",
       "  '',\n",
       "  'tan',\n",
       "  'than',\n",
       "  '',\n",
       "  'toad',\n",
       "  'ton',\n",
       "  'torn',\n",
       "  'town',\n",
       "  'Ulan',\n",
       "  '',\n",
       "  'van',\n",
       "  'wan',\n",
       "  'wean',\n",
       "  'woman',\n",
       "  'won',\n",
       "  'worn',\n",
       "  'Wotan',\n",
       "  'yon',\n",
       "  'Zan',\n",
       "  'zeal',\n",
       "  '',\n",
       "  'zonal',\n",
       "  'zone',\n",
       "  'zoo',\n",
       "  'zoom',\n",
       "  'zoos',\n",
       "  'Zorn')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "z = list(zip(*lis))\n",
    "\n",
    "z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can filter out the empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['abrogating', 'arrogating'],\n",
       " ['abandoned'],\n",
       " ['abbreviate'],\n",
       " ['abstracted', 'attracted', 'distracted', 'extracted', 'retracted'],\n",
       " ['absentee', 'absinthe'],\n",
       " ['abased',\n",
       "  'abashed',\n",
       "  'abused',\n",
       "  'abuses',\n",
       "  'abutted',\n",
       "  'abysses',\n",
       "  'amassed',\n",
       "  'ambushed',\n",
       "  'amused',\n",
       "  'bossed',\n",
       "  'bused',\n",
       "  'busied',\n",
       "  'bussed',\n",
       "  'busses',\n",
       "  'busted'],\n",
       " ['abase',\n",
       "  'abash',\n",
       "  'abbe',\n",
       "  'abed',\n",
       "  'abet',\n",
       "  'abets',\n",
       "  'able',\n",
       "  'ably',\n",
       "  'Abos',\n",
       "  'about',\n",
       "  'Abu',\n",
       "  'abuse',\n",
       "  'abut',\n",
       "  'abuts',\n",
       "  'abyss',\n",
       "  'aces',\n",
       "  'adds',\n",
       "  'ads',\n",
       "  'ages',\n",
       "  'ague',\n",
       "  'aids',\n",
       "  'aims',\n",
       "  'airs',\n",
       "  'alas',\n",
       "  'album',\n",
       "  'albums',\n",
       "  'alms',\n",
       "  'alum',\n",
       "  'ambush',\n",
       "  'amuse',\n",
       "  'ants',\n",
       "  'anus',\n",
       "  'apes',\n",
       "  'aqua',\n",
       "  'arcs',\n",
       "  'arms',\n",
       "  'arts',\n",
       "  'as',\n",
       "  'asks',\n",
       "  'ass',\n",
       "  'awls',\n",
       "  'axes',\n",
       "  'axis',\n",
       "  'ayes',\n",
       "  'babes',\n",
       "  'Babul',\n",
       "  'bud',\n",
       "  'buds',\n",
       "  'bug',\n",
       "  'bugs',\n",
       "  'bum',\n",
       "  'bums',\n",
       "  'bun',\n",
       "  'buns',\n",
       "  'bus',\n",
       "  'bush',\n",
       "  'buss',\n",
       "  'bust',\n",
       "  'busy',\n",
       "  'but',\n",
       "  'buy',\n",
       "  'buys',\n",
       "  'cabs',\n",
       "  'deus',\n",
       "  'ebbs',\n",
       "  'Gus',\n",
       "  'ibis',\n",
       "  'jabs',\n",
       "  'Janus',\n",
       "  'Kabul',\n",
       "  'labs',\n",
       "  'onus',\n",
       "  'opus',\n",
       "  'Pius',\n",
       "  'plus',\n",
       "  'pus',\n",
       "  'Sus',\n",
       "  'tabs',\n",
       "  'thus',\n",
       "  'us',\n",
       "  'Zeus'],\n",
       " ['Alan',\n",
       "  'an',\n",
       "  'ban',\n",
       "  'bean',\n",
       "  'boa',\n",
       "  'boar',\n",
       "  'boat',\n",
       "  'Bonn',\n",
       "  'boon',\n",
       "  'born',\n",
       "  'bran',\n",
       "  'can',\n",
       "  'clan',\n",
       "  'coal',\n",
       "  'coat',\n",
       "  'coax',\n",
       "  'Cohn',\n",
       "  'coin',\n",
       "  'con',\n",
       "  'coon',\n",
       "  'corn',\n",
       "  'Cowan',\n",
       "  'Dan',\n",
       "  'dean',\n",
       "  'Dolan',\n",
       "  'don',\n",
       "  'down',\n",
       "  'Egan',\n",
       "  'fan',\n",
       "  'foal',\n",
       "  'foam',\n",
       "  'Fran',\n",
       "  'Goa',\n",
       "  'goad',\n",
       "  'goal',\n",
       "  'goat',\n",
       "  'gown',\n",
       "  'groan',\n",
       "  'Han',\n",
       "  'hoar',\n",
       "  'Hokan',\n",
       "  'horn',\n",
       "  'Ian',\n",
       "  'ion',\n",
       "  'Iran',\n",
       "  'Ivan',\n",
       "  'jean',\n",
       "  'Joan',\n",
       "  'John',\n",
       "  'join',\n",
       "  'Jon',\n",
       "  'Juan',\n",
       "  'Klan',\n",
       "  'Koran',\n",
       "  'lean',\n",
       "  'load',\n",
       "  'loaf',\n",
       "  'loan',\n",
       "  'loans',\n",
       "  'Logan',\n",
       "  'loin',\n",
       "  'loon',\n",
       "  'man',\n",
       "  'mean',\n",
       "  'moan',\n",
       "  'moans',\n",
       "  'moat',\n",
       "  'Moen',\n",
       "  'Moon',\n",
       "  'Moran',\n",
       "  'morn',\n",
       "  'Nan',\n",
       "  'Noah',\n",
       "  'Nolan',\n",
       "  'non',\n",
       "  'noon',\n",
       "  'noun',\n",
       "  'oaf',\n",
       "  'oak',\n",
       "  'oar',\n",
       "  'oat',\n",
       "  'Oman',\n",
       "  'on',\n",
       "  'own',\n",
       "  'pan',\n",
       "  'plan',\n",
       "  'ran',\n",
       "  'road',\n",
       "  'roam',\n",
       "  'roar',\n",
       "  'Roman',\n",
       "  'Ron',\n",
       "  'Ryan',\n",
       "  'San',\n",
       "  'scan',\n",
       "  'Sean',\n",
       "  'Sian',\n",
       "  'Sloan',\n",
       "  'soak',\n",
       "  'soap',\n",
       "  'soar',\n",
       "  'son',\n",
       "  'soon',\n",
       "  'sown',\n",
       "  'span',\n",
       "  'Stan',\n",
       "  'swan',\n",
       "  'tan',\n",
       "  'than',\n",
       "  'toad',\n",
       "  'ton',\n",
       "  'torn',\n",
       "  'town',\n",
       "  'Ulan',\n",
       "  'van',\n",
       "  'wan',\n",
       "  'wean',\n",
       "  'woman',\n",
       "  'won',\n",
       "  'worn',\n",
       "  'Wotan',\n",
       "  'yon',\n",
       "  'Zan',\n",
       "  'zeal',\n",
       "  'zonal',\n",
       "  'zone',\n",
       "  'zoo',\n",
       "  'zoom',\n",
       "  'zoos',\n",
       "  'Zorn']]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "z = [ list(filter(lambda s: s!= '', L)) for L in z]\n",
    "\n",
    "z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not entirely satisfactory, but it might work well for correcting the spelling of small sets of words, for example, state names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple captures\n",
    "It's now possible to obtain information on all the successful matches of a repeated capture group, not just the last one.  Use **captures** instead of **group**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atgcgcattcgggcgtga']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['cgc', 'att', 'cgg', 'gcg']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna = 'cgcgcATGcgcattcgggcgTGAcgcgcgTAGcgcgcgcgc'\n",
    "dna = dna.lower()\n",
    "orfpat = r'(?x) ( atg ( (?!tga|tag|taa) ... )*  (?:tga|tag|taa))'\n",
    "search(orfpat,dna).captures(1)\n",
    "search(orfpat,dna).captures(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atgcgcattcgggcgtga']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(orfpat,dna).captures(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also capture things by name.  The string **s** is an excerpt of a long file describing a gene network.  Each line contains two gene names, and the strength of the connection between them.  In this example, we are only interested in gathering the gene names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'geneA': ['AT1G01280', 'AT1G01480', 'AT1G01600', 'AT1G01430', 'AT1G01150'],\n",
       " 'geneB': ['AT1G01450', 'AT1G01560', 'AT1G01610', 'AT1G01630', 'AT1G01700']}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"\"\"AT1G01280\tAT1G01450\t5.1E-3\n",
    "AT1G01480\tAT1G01560\t2.3E-2\n",
    "AT1G01600\tAT1G01610\t1.6E-2\n",
    "AT1G01430\tAT1G01630\t2.1E-2\n",
    "AT1G01150\tAT1G01700\t1.1E-2\n",
    "\"\"\"\n",
    "m = match(\n",
    "    r'(?x) (?: (?P<geneA>\\w+) \\s+ (?P<geneB>\\w+) \\s+ \\S+ \\n )*',\n",
    "    s)\n",
    "m.capturesdict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even reuse a name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gene': ['AT1G01280',\n",
       "  'AT1G01450',\n",
       "  'AT1G01480',\n",
       "  'AT1G01560',\n",
       "  'AT1G01600',\n",
       "  'AT1G01610',\n",
       "  'AT1G01430',\n",
       "  'AT1G01630',\n",
       "  'AT1G01150',\n",
       "  'AT1G01700']}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m = match(\n",
    "    r'(?x) (?: (?P<gene>\\w+) \\s+ (?P<gene>\\w+) \\s+ \\S+ \\n )*',\n",
    "    s)\n",
    "\n",
    "m.capturesdict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AT1G01150',\n",
       " 'AT1G01280',\n",
       " 'AT1G01430',\n",
       " 'AT1G01450',\n",
       " 'AT1G01480',\n",
       " 'AT1G01560',\n",
       " 'AT1G01600',\n",
       " 'AT1G01610',\n",
       " 'AT1G01630',\n",
       " 'AT1G01700']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(m.capturesdict()['gene']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse searching\n",
    "\n",
    "Searches can now work backwards:\n",
    "\n",
    "Note: the result of a reverse search is not necessarily the reverse of a forward search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de', 'bc']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findall(r\"(?r)..\", \"abcde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who cares?  I thought of an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 mile = 1,760 yards = 5,280 ft = 63,360 in = 1,609,344 mm = ,160,934.4 cm, more or less.  Pi = 3.14,159'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub(r'(?rx) (\\d\\d\\d)',\n",
    "    r',\\1',\n",
    "    '1 mile = 1760 yards = 5280 ft = 63360 in = 1609344 mm = 160934.4 cm, more or less.  Pi = 3.14159')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 mile = 1,760 yards = 5,280 ft = 63,360 in = 1,609,344 mm = 160,934.4 cm, more or less.  Pi = 3.14,159'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub(r'(?rx)  (?<=\\d) (\\d\\d\\d)',\n",
    "    r',\\1',\n",
    "    '1 mile = 1760 yards = 5280 ft = 63360 in = 1609344 mm = 160934.4 cm, more or less.  Pi = 3.14159')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 mile = 1,760 yards = 5,280 ft = 63,360 in = 1,609,344 mm = 160,934.4 cm, more or less.  Pi = 3.14159'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub(r'(?rx)  (?<! [.] \\d*) (?<=\\d) (\\d\\d\\d)',\n",
    "    r',\\1',\n",
    "    '1 mile = 1760 yards = 5280 ft = 63360 in = 1609344 mm = 160934.4 cm, more or less.  Pi = 3.14159')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSIX Matching (Leftmost Longest)\n",
    "\n",
    "The default matching method for alternations is to match the first alternative that will match. The POSIX standard is to find the leftmost longest match. This can be turned on using the POSIX flag **(?p)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<regex.Match object; span=(4, 7), match='dog'>,\n",
       " <regex.Match object; span=(27, 30), match='dog'>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(finditer( r'(dog|doge|doggerel)', 'The doge wrote nothing but doggerel.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<regex.Match object; span=(4, 8), match='doge'>,\n",
       " <regex.Match object; span=(27, 35), match='doggerel'>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "list(finditer( r'(?p)(dog|doge|doggerel)', 'The doge wrote nothing but doggerel.'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **fullmatch**\n",
    "The pattern must match the entire string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<regex.Match object; span=(0, 8), match='The doge'>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "match(r'The doge', 'The doge wrote nothing but doggerel.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fullmatch(r'The doge', 'The doge wrote nothing but doggerel.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, that one didn't match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial matching\n",
    "Can the target string be extended to match the pattern?  The optional **partial** argument to **match**, **search**, and **fullmatch** can answer this question. This could be useful if you are validating input from the terminal, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is true, because the target can be extended to 'The doge wrote nothing but doggerel.' to match the pattern.  But, if you think about it, you will see that this one is true for any target string.  (It can be extended with 'dogdog'.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<regex.Match object; span=(0, 22), match='The doge wrote nothing', partial=True>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullmatch(r'.*dog.*dog.*', 'The doge wrote nothing', partial=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is more interesting: Can the string be extended to be a Social Security Number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<regex.Match object; span=(0, 8), match='999-89-7', partial=True>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullmatch(r'\\d\\d\\d-\\d\\d-\\d\\d\\d\\d',  \"999-89-7\", partial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<regex.Match object; span=(0, 8), match='999-89-7', partial=True>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match(r'\\d\\d\\d-\\d\\d-\\d\\d\\d\\d',  \"999-89-7\", partial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fullmatch(r'\\d\\d\\d-\\d\\d-\\d\\d\\d\\d',  \"My SSN is 999-89-7\", partial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match(r'\\d\\d\\d-\\d\\d-\\d\\d\\d\\d',  \"My SSN is 999-89-7\", partial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<regex.Match object; span=(10, 18), match='999-89-7', partial=True>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(r'\\d\\d\\d-\\d\\d-\\d\\d\\d\\d',  \"My SSN is 999-89-7\", partial=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this one is a complete match, so the **partial** field is missing from the match object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<regex.Match object; span=(10, 21), match='999-89-7654'>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(r'\\d\\d\\d-\\d\\d-\\d\\d\\d\\d',  \"My SSN is 999-89-7654, but don't tell.\", partial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<regex.Match object; span=(36, 36), match='', partial=True>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(r'\\d\\d\\d-\\d\\d-\\d\\d\\d\\d',  \"My SSN is 999-89-76, but don't tell.\", partial=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some functional programming fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam spam spam spam spam spam spam spam eggs and spam'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def twice(f):\n",
    "    return lambda x: f(f(x))\n",
    "\n",
    "def prepender(s):\n",
    "    return lambda t: s + t\n",
    "\n",
    "twice(twice)(twice(prepender('spam ')))('eggs and spam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "twice(twice)(twice(prepender(len('spam '))))(len('eggs and spam'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puzzle\n",
    "Which character is most frequent in a string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2', 5)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most(s, care_about=r'\\w'):\n",
    "    t=''.join(sorted(s))\n",
    "    p = r'((' + care_about + r')\\2*)'\n",
    "    L = [ m.group(1) for m in finditer(p, t) ]\n",
    "    m = max(L, key=len)\n",
    "    return (m[0], len(m))\n",
    "\n",
    "most('123462232340997092')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3', 3)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most('123462232340997092', care_about='[13579]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 10)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most(twice(twice)(twice(prepender('spam ')))('eggs and spam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s', 10)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most(twice(twice)(twice(prepender('spam ')))('eggs and spam'), r'[^aeiou\\s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
